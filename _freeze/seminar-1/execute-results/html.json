{
  "hash": "83be33ae44849d65a71df84da9186182",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Seminar 1: Power Calculations and Simulations\"\nformat:\n  html:\n    toc: true              # Include a table of contents in the HTML output\n    toc-depth: 2           # Set the depth of the table of contents\n    number-sections: true  # Number the sections in the HTML output\n    css: styles.css        # Optionally include custom CSS for styling\n    resources: \n      - \"seminar-1.pdf\"     # Make the PDF file available for download\n    #code-fold: true       # places code in dropdown\n  pdf:\n    #documentclass: article # LaTeX class used for the PDF document\n    toc: true              # Include a table of contents in the PDF output\n    number-sections: true  # Number the sections in the PDF output\n    keep-md: false          # Optionally keep the intermediate markdown file\n    output-file: \"seminar-1.pdf\" # Specify the PDF output file name\n    echo: true            # unhides code\n---\n\n\n## Required R Packages\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(modelsummary)\nlibrary(tinytable)\nlibrary(psych)\n```\n:::\n\n\n## Learning Objectives\n\nIn this seminar you will learn how to:\n\n-   make an *a priori* power calculation for an experiment;\n\n-   simulate an experiment in R/Stata;\n\n-   and visualize the properties of the power function.\n\n## Motivation\n\nStatistical power is an valuable theoretical and practical concept. The power of a test is the probability of rejecting the null hypothesis when the alternative hypothesis is true.\n\nLet's consider the simple null hypothesis of no *average* treatment effect,[^1]\n\n[^1]: This is not the same as Fisher's sharp null hypothesis that the unit-level treatment effect is 0 for all units in the data.\n\n$$\n  H_0: \\tau_{ATE} = 0\n$$\n\nWhen testing this null hypothesis against the alternative,\n\n$$\nH_1: \\tau_{ATE} \\neq 0\n$$\n\nthe set of values that fulfill the alternative hypothesis is any value on the real line, excluding 0.\n\nThe power of this test is,\n\n$$\n  Pr\\big(\\text{Reject }H_0|H_1\\text{ is true}\\big) = Pr\\big(\\text{Reject }H_0|\\tau_{ATE} = \\tau_0; \\tau_0\\neq 0\\big)\n$$\n\nWhen do we reject $H_0$? A valid test is defined by a rejection rule based on a test static and a corresponding critical value. For example, in this two-sided test,\n\n$$\n\\text{Reject }H_0\\text{ if}\\qquad \\bigg| \\frac{\\hat{\\tau}}{se(\\hat{\\tau})}\\bigg|>z_{1-\\alpha/2}\n$$\n\nIn statistical inference there is a trade off between Type I and II errors. We limit Type I erros - probability of rejecting $H_0$ when $H_0$ is true - through the choice of $\\alpha$: the significance level. In Economics, this is typically chosen to be 1%, 5%, or 10%.\n\n$$\n\\alpha = Pr\\big(\\text{Reject }H_0|H_0\\text{ is true}\\big)\n$$ A test is said to be of *size* $\\alpha$ if the probability of a Type I error is $\\leq \\alpha$.\n\nThe finite sample distribution of this test statistic is not known as we do not know the distributions of $\\{Y(1),Y(0)\\}$ upon which the test statistic is based.[^2] By the Central Limit Theorem, we know that the limiting distribution is standard normal under the null hypothesis. This is true only under $H_0$. The CLT holds for the true mean of estimator ($E[\\hat{\\tau}]=\\tau_{ATE}$), which we know under $H_0$ to be $0$.\n\n[^2]: Those familiar with the Classical Linear Regression Model might find this surprising. Recall, one of the assumption of the CLRM is that the error term is normally distributed. This assumption means that even in a finite sample, the distribution of the estimator (or test statistic) is normally distributed. When the variance of the error term is not known, and must be estimated, the resulting test statistic is T-distributed.\n\nThus, under $H_0$,\n\n$$\n\\frac{\\hat{\\tau}}{se(\\hat{\\tau})} = \\frac{\\hat{\\tau}-E[\\hat{\\tau}]}{se(\\hat{\\tau})} \\overset{d}\\longrightarrow N(0,1) \\quad \\text{as }n\\rightarrow \\infty\n$$ For this reason, we can approximate the distribution of the test statistic with the standard normal.\n\n$$\n\\frac{\\hat{\\tau}}{se(\\hat{\\tau})} \\quad a.\\thicksim N(0,1) \\quad \\text{under }H_0\n$$\n\n::: {#exr-size1}\nCan you show that the size of this test is $\\alpha$ (as $n\\rightarrow \\infty$)?\n:::\n\nUnfortunatley, we cannot restrict the probability of both a Type I and II error. Here, we chose the critical values to give us a test of size $\\alpha$. By doing so, we give up the possibility of limiting the probability of a Type II error. Moreover, a smaller $\\alpha$ will necessarily imply a higher probability of Type II error.[^3]\n\n[^3]: In the limit, we can reduce the probability of a Type I error to 0 by setting the rejection rule to \"never reject\". As a result, the probability of a Type II error will be 1 as you will always fail to reject $H_0$ when it is false.\n\nThe probability of a Type II error depends on the power of the test.\n\n$$\nPr\\big(\\text{Failing to reject }H_0|H_0\\text{ is false}\\big) = 1 - Pr\\big(\\text{Reject }H_0|H_1\\text{ is true}\\big)\n$$\n\nA more powerful test has a lower probability of Type II error. Since, two tests might have the same size (i.e., $\\alpha$) we strictly prefer the more powerful test. Power is therefore an important metric by which we can compare different tests.\n\nFor this test ($H_0: \\tau_{ATE}=0$), the power of the test tells you how likely you are to detect a given non-zero average treatment effect in the experiment. Given how expensive it is to implement experiments (especially in Economics), this is useful information. There would be no point running a RCT if the likelihood of rejecting a null effect is very low.\n\n## Power calculation\n\nIn this setting, the power function tells you the probability of rejecting $H_0$ for any true value of the ATE; denoted here by the scalar $\\tau_0$. For the above two-sided test, this is given by,\n\n$$\n\\begin{aligned}\n        \\theta(\\tau_0)=&Pr(\\text{Reject }H_0|\\tau_{ATE}=\\tau_0) \\\\\n        =& Pr\\left(\\bigg|\\frac{\\hat{\\tau}}{se(\\hat{\\tau})}\\bigg|>z_{1-\\alpha/2}\\bigg|\\tau_{ATE}=\\tau_0\\right) \\\\\n        =&Pr\\left(\\frac{\\hat{\\tau}}{se(\\hat{\\tau})}<z_{\\alpha/2}\\bigg|\\tau_{ATE}=\\tau_0\\right)+Pr\\left(\\frac{\\hat{\\tau}}{se(\\hat{\\tau})}>z_{1-\\alpha/2}\\bigg|\\tau_{ATE}=\\tau_0\\right) \\\\\n        =&Pr\\left(\\frac{\\hat{\\tau}-\\tau_0}{se(\\hat{\\tau})}+\\frac{\\tau_0}{se(\\hat{\\tau})}<z_{\\alpha/2}\\bigg|\\tau_{ATE}=\\tau_0\\right)+Pr\\left(\\frac{\\hat{\\tau}-\\tau_0}{se(\\hat{\\tau})}+\\frac{\\tau_0}{se(\\hat{\\tau})}>z_{1-\\alpha/2}\\bigg|\\tau_{ATE}=\\tau_0\\right) \\\\\n        \\approx& Pr\\left(Z<z_{\\alpha/2}-\\frac{\\tau_0}{se(\\hat{\\tau})}\\bigg|\\tau_{ATE}=\\tau_0\\right)+Pr\\left(Z>z_{1-\\alpha/2}-\\frac{\\tau_0}{se(\\hat{\\tau})}\\bigg|\\tau_{ATE}=\\tau_0\\right)\n    \\end{aligned}\n$$\n\nwhere $Z$ is a standard normal random variable. The final equality is an approximation since we do not know the finite sample distribution of the test statistic.\n\n::: {#exr-power1}\nGiven the above definition of the power function, what is $\\theta(0)$?\n:::\n\nTo make further use of this function, we need to define $\\hat{\\tau}$ and $se(\\hat{\\tau})$. Consider,\n\n$$\n\\begin{aligned}\n  \\hat{\\tau} =& \\frac{1}{N_t} \\sum_{i:W_i=1}Y_i-\\frac{1}{N_c} \\sum_{i:W_i=0}Y_i \\\\\n  se(\\hat{\\tau}) =& \\sqrt{V^{cons}} \\\\\n  =&\\sqrt{\\sigma^2\\left(\\frac{1}{N_t}+\\frac{1}{N_c}\\right)}\n\\end{aligned}\n$$\n\nHere we are using the constant-variance finite sample estimator. One cannot use the Neyman estimator without prior knowledge the variance of $Y(1)$, the potential outcome with treatment. For this reason, it is less useful in power calculations made prior to the experiment.[^4]\n\n[^4]: You could use estimates from a previous study.\n\n::: {#exr-power2}\nA researcher has a sample of 650 individuals: $N_t = 270$ and $N_c = 380$. Using the above formulae, calculate the power of the experiment to detect a treatment effect equal to 15% of a standard deviation of the outcome variable.\n:::\n\n::: {#exr-power3}\nA researcher needs to show that their test has 80% power for a treatment effect of 20% of a standard deviation of the outcome. Using the above formulae, compute the sample size needed to achieve this level of power when assignment into treatment is equal. Then compute the required sample size if only a third of the sample is treated.\n:::\n\n::: {#exr-power4}\nHow could you improve the power of an experiment?\n:::\n\n## Simulation of power function\n\nIn this section we will simulate an experiment and plot the power function of the OLS estimator from a simple linear regression model.[^5] We will also examine the impact of adding good controls (covariates) to the estimated model.\n\n[^5]: Recall from lectures, the $\\hat{\\beta}$-OLS estimator for a simple univariate regression model (\\$Y_i = \\\\alpha + \\\\beta W_i + \\\\varepsilon_i\\$), is the same as the above $\\hat{\\tau}$ estimator.\n\n### Setup\n\nWe begin by constructing a vector of potential outcomes, $\\{Y(0),Y(1)\\}$, based on a known data generating process (DGP) and treatment effect. We will assign values to $Y(0)$ based on the following linear DGP,\n\n$$\n        Y_i(0) = \\alpha + X_i'\\gamma + \\varepsilon_i\n$$\n\nwhere,\n\n-   $Y_i(0)$ is the potential outcome without treatment and is measured after treatment has taken place. In this exercise we assume that the outcome is the log of wages.\n\n-   $X_i$ is a vector of covariates that explain some of the variation in $Y_i(0)$. In this instance, we will treat it as a linear term in age and gender dummy.\n\n-   The error term will be drawn from a distribution of our choosing.\n\nFor this exercise, the relevant parameters $\\{\\alpha,\\gamma',\\sigma_{\\varepsilon}\\}$ have been chosen to match a linear model of log wages against a dummy variable for gender and linear term in age, using data from the 2017 US CPS.\n\nWe can then generate the observed outcome as,\n\n$$\n    Y_i = Y_i(0) + \\tau W_i\n$$\n\nwhere $W_i$ is an indicator function denoting treatment status in a completely randomized experiment with equal allocation to treatment and control. We will specify the homogeneous treatment to be $\\tau = 2\\%$.[^6]\n\n[^6]: This assumption simplifies the simulation by removing any need to consider bias related to heterogeneity.\n\n### Data generating process\n\nStart by setting a seed. This ensures that all random number generators in this programme are replicable. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(12956)\n```\n:::\n\n\nGenerate an empty dataframe with 200 observations.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata1 <- data.frame(matrix(ncol = 0, nrow = 200))\n```\n:::\n\n\nDefine the parameters of the simulation.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nalpha <- 0.07\ngamma_f <- -0.17\ngamma_a <- 0.08\ntau <- 0.02\n```\n:::\n\n\n\nAfter having selected each parameter we need to draw values of each explanatory variable, including the error term, in order to calculate the value of the potential outcomes. In this instance we will make use of known distributions. \n \nThe error term will be drawn from a normal distribution $N(0,0.55)$. Gender will be drawn from a binonmial distribution while age will be a sequence of integers assigned based on a uniform distribution. As each variable is drawn independently, the covariance of the two variables should be zero. However, the covariance in the realized sample will be non-zero. \n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata1$female <- rbinom(200,1,0.5)\ndata1$age <- floor(46*runif(200) + 20)\ndata1$error <- rnorm(200,0,0.55)\n```\n:::\n\n\nGenerate Y(0)\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata1$Y0 <- alpha + gamma_f*data1$female + gamma_a*data1$age + data1$error\n```\n:::\n\n\nTo control the number of observations assigned to treatment and control we will not use the binomial distribution command. Instead we will use a uniformly distributed number to sort and then group the data. Note, there are many way to do this assignment. \n\nWe can then apply the treatment and generate the observable outcome variable.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata1$temp <- runif(200)\ndata1$treat <- ifelse(data1$temp>=median(data1$temp),1,0)\ntable(data1$treat)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n  0   1 \n100 100 \n```\n\n\n:::\n:::\n\n\nGenerate the observable outcome variable\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata1$Yobs <- data1$Y0 + tau*data1$treat\n```\n:::\n\n\nWe can now estimate the treatment effect using a linear regression model, with and without the additional covariates.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nreg1 <- lm(Yobs ~ treat, data=data1)\nreg2 <- lm(Yobs ~ treat + female + age, data=data1)\nmodelsummary(list(\"(1)\"=reg1,\"(2)\"=reg2), stars=c('*'=.1, '**'=.05,'***'=.01), gof_map = c(\"nobs\", \"r.squared\", \"rmse\"))\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<!DOCTYPE html> \n<html lang=\"en\">\n  <head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>tinytable_6pi48zj88msnj88p7u2w</title>\n    <style>\n.table td.tinytable_css_p1vi9iln1z03xbjlzqyx, .table th.tinytable_css_p1vi9iln1z03xbjlzqyx {    border-bottom: solid 0.1em #d3d8dc; }\n.table td.tinytable_css_wxqs2ozpclre9iwhurnm, .table th.tinytable_css_wxqs2ozpclre9iwhurnm {    text-align: left; }\n.table td.tinytable_css_w8k4q721s4jlejjopaiv, .table th.tinytable_css_w8k4q721s4jlejjopaiv {    text-align: center; }\n.table td.tinytable_css_4l8vlgkxxw42fztnfv8d, .table th.tinytable_css_4l8vlgkxxw42fztnfv8d {    text-align: center; }\n.table td.tinytable_css_q3g1zoy7myqw2jgelyvs, .table th.tinytable_css_q3g1zoy7myqw2jgelyvs {    border-bottom: solid 0.05em black; }\n    </style>\n    <script src=\"https://polyfill.io/v3/polyfill.min.js?features=es6\"></script>\n    <script id=\"MathJax-script\" async src=\"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"></script>\n    <script>\n    MathJax = {\n      tex: {\n        inlineMath: [['$', '$'], ['\\\\(', '\\\\)']]\n      },\n      svg: {\n        fontCache: 'global'\n      }\n    };\n    </script>\n  </head>\n\n  <body>\n    <div class=\"container\">\n      <table class=\"table table-borderless\" id=\"tinytable_6pi48zj88msnj88p7u2w\" style=\"width: auto; margin-left: auto; margin-right: auto;\" data-quarto-disable-processing='true'>\n        <thead>\n        \n              <tr>\n                <th scope=\"col\"> </th>\n                <th scope=\"col\">(1)</th>\n                <th scope=\"col\">(2)</th>\n              </tr>\n        </thead>\n        <tfoot><tr><td colspan='3'>* p < 0.1, ** p < 0.05, *** p < 0.01</td></tr></tfoot>\n        <tbody>\n                <tr>\n                  <td>(Intercept)</td>\n                  <td>3.397***</td>\n                  <td>0.094   </td>\n                </tr>\n                <tr>\n                  <td>           </td>\n                  <td>(0.120) </td>\n                  <td>(0.146) </td>\n                </tr>\n                <tr>\n                  <td>treat      </td>\n                  <td>-0.018  </td>\n                  <td>-0.003  </td>\n                </tr>\n                <tr>\n                  <td>           </td>\n                  <td>(0.170) </td>\n                  <td>(0.082) </td>\n                </tr>\n                <tr>\n                  <td>female     </td>\n                  <td>        </td>\n                  <td>-0.182**</td>\n                </tr>\n                <tr>\n                  <td>           </td>\n                  <td>        </td>\n                  <td>(0.083) </td>\n                </tr>\n                <tr>\n                  <td>age        </td>\n                  <td>        </td>\n                  <td>0.079***</td>\n                </tr>\n                <tr>\n                  <td>           </td>\n                  <td>        </td>\n                  <td>(0.003) </td>\n                </tr>\n                <tr>\n                  <td>Num.Obs.   </td>\n                  <td>200     </td>\n                  <td>200     </td>\n                </tr>\n                <tr>\n                  <td>R2         </td>\n                  <td>0.000   </td>\n                  <td>0.770   </td>\n                </tr>\n                <tr>\n                  <td>RMSE       </td>\n                  <td>1.20    </td>\n                  <td>0.57    </td>\n                </tr>\n        </tbody>\n      </table>\n    </div>\n\n    <script>\n      function styleCell_tinytable_9gs4ubkyn36sjtay2wq6(i, j, css_id) {\n        var table = document.getElementById(\"tinytable_6pi48zj88msnj88p7u2w\");\n        table.rows[i].cells[j].classList.add(css_id);\n      }\n      function insertSpanRow(i, colspan, content) {\n        var table = document.getElementById('tinytable_6pi48zj88msnj88p7u2w');\n        var newRow = table.insertRow(i);\n        var newCell = newRow.insertCell(0);\n        newCell.setAttribute(\"colspan\", colspan);\n        // newCell.innerText = content;\n        // this may be unsafe, but innerText does not interpret <br>\n        newCell.innerHTML = content;\n      }\n      function spanCell_tinytable_9gs4ubkyn36sjtay2wq6(i, j, rowspan, colspan) {\n        var table = document.getElementById(\"tinytable_6pi48zj88msnj88p7u2w\");\n        const targetRow = table.rows[i];\n        const targetCell = targetRow.cells[j];\n        for (let r = 0; r < rowspan; r++) {\n          // Only start deleting cells to the right for the first row (r == 0)\n          if (r === 0) {\n            // Delete cells to the right of the target cell in the first row\n            for (let c = colspan - 1; c > 0; c--) {\n              if (table.rows[i + r].cells[j + c]) {\n                table.rows[i + r].deleteCell(j + c);\n              }\n            }\n          }\n          // For rows below the first, delete starting from the target column\n          if (r > 0) {\n            for (let c = colspan - 1; c >= 0; c--) {\n              if (table.rows[i + r] && table.rows[i + r].cells[j]) {\n                table.rows[i + r].deleteCell(j);\n              }\n            }\n          }\n        }\n        // Set rowspan and colspan of the target cell\n        targetCell.rowSpan = rowspan;\n        targetCell.colSpan = colspan;\n      }\n\nwindow.addEventListener('load', function () { styleCell_tinytable_9gs4ubkyn36sjtay2wq6(0, 0, 'tinytable_css_p1vi9iln1z03xbjlzqyx') })\nwindow.addEventListener('load', function () { styleCell_tinytable_9gs4ubkyn36sjtay2wq6(0, 1, 'tinytable_css_p1vi9iln1z03xbjlzqyx') })\nwindow.addEventListener('load', function () { styleCell_tinytable_9gs4ubkyn36sjtay2wq6(0, 2, 'tinytable_css_p1vi9iln1z03xbjlzqyx') })\nwindow.addEventListener('load', function () { styleCell_tinytable_9gs4ubkyn36sjtay2wq6(0, 0, 'tinytable_css_wxqs2ozpclre9iwhurnm') })\nwindow.addEventListener('load', function () { styleCell_tinytable_9gs4ubkyn36sjtay2wq6(1, 0, 'tinytable_css_wxqs2ozpclre9iwhurnm') })\nwindow.addEventListener('load', function () { styleCell_tinytable_9gs4ubkyn36sjtay2wq6(2, 0, 'tinytable_css_wxqs2ozpclre9iwhurnm') })\nwindow.addEventListener('load', function () { styleCell_tinytable_9gs4ubkyn36sjtay2wq6(3, 0, 'tinytable_css_wxqs2ozpclre9iwhurnm') })\nwindow.addEventListener('load', function () { styleCell_tinytable_9gs4ubkyn36sjtay2wq6(4, 0, 'tinytable_css_wxqs2ozpclre9iwhurnm') })\nwindow.addEventListener('load', function () { styleCell_tinytable_9gs4ubkyn36sjtay2wq6(5, 0, 'tinytable_css_wxqs2ozpclre9iwhurnm') })\nwindow.addEventListener('load', function () { styleCell_tinytable_9gs4ubkyn36sjtay2wq6(6, 0, 'tinytable_css_wxqs2ozpclre9iwhurnm') })\nwindow.addEventListener('load', function () { styleCell_tinytable_9gs4ubkyn36sjtay2wq6(7, 0, 'tinytable_css_wxqs2ozpclre9iwhurnm') })\nwindow.addEventListener('load', function () { styleCell_tinytable_9gs4ubkyn36sjtay2wq6(8, 0, 'tinytable_css_wxqs2ozpclre9iwhurnm') })\nwindow.addEventListener('load', function () { styleCell_tinytable_9gs4ubkyn36sjtay2wq6(9, 0, 'tinytable_css_wxqs2ozpclre9iwhurnm') })\nwindow.addEventListener('load', function () { styleCell_tinytable_9gs4ubkyn36sjtay2wq6(10, 0, 'tinytable_css_wxqs2ozpclre9iwhurnm') })\nwindow.addEventListener('load', function () { styleCell_tinytable_9gs4ubkyn36sjtay2wq6(11, 0, 'tinytable_css_wxqs2ozpclre9iwhurnm') })\nwindow.addEventListener('load', function () { styleCell_tinytable_9gs4ubkyn36sjtay2wq6(0, 1, 'tinytable_css_w8k4q721s4jlejjopaiv') })\nwindow.addEventListener('load', function () { styleCell_tinytable_9gs4ubkyn36sjtay2wq6(1, 1, 'tinytable_css_w8k4q721s4jlejjopaiv') })\nwindow.addEventListener('load', function () { styleCell_tinytable_9gs4ubkyn36sjtay2wq6(2, 1, 'tinytable_css_w8k4q721s4jlejjopaiv') })\nwindow.addEventListener('load', function () { styleCell_tinytable_9gs4ubkyn36sjtay2wq6(3, 1, 'tinytable_css_w8k4q721s4jlejjopaiv') })\nwindow.addEventListener('load', function () { styleCell_tinytable_9gs4ubkyn36sjtay2wq6(4, 1, 'tinytable_css_w8k4q721s4jlejjopaiv') })\nwindow.addEventListener('load', function () { styleCell_tinytable_9gs4ubkyn36sjtay2wq6(5, 1, 'tinytable_css_w8k4q721s4jlejjopaiv') })\nwindow.addEventListener('load', function () { styleCell_tinytable_9gs4ubkyn36sjtay2wq6(6, 1, 'tinytable_css_w8k4q721s4jlejjopaiv') })\nwindow.addEventListener('load', function () { styleCell_tinytable_9gs4ubkyn36sjtay2wq6(7, 1, 'tinytable_css_w8k4q721s4jlejjopaiv') })\nwindow.addEventListener('load', function () { styleCell_tinytable_9gs4ubkyn36sjtay2wq6(8, 1, 'tinytable_css_w8k4q721s4jlejjopaiv') })\nwindow.addEventListener('load', function () { styleCell_tinytable_9gs4ubkyn36sjtay2wq6(9, 1, 'tinytable_css_w8k4q721s4jlejjopaiv') })\nwindow.addEventListener('load', function () { styleCell_tinytable_9gs4ubkyn36sjtay2wq6(10, 1, 'tinytable_css_w8k4q721s4jlejjopaiv') })\nwindow.addEventListener('load', function () { styleCell_tinytable_9gs4ubkyn36sjtay2wq6(11, 1, 'tinytable_css_w8k4q721s4jlejjopaiv') })\nwindow.addEventListener('load', function () { styleCell_tinytable_9gs4ubkyn36sjtay2wq6(0, 2, 'tinytable_css_4l8vlgkxxw42fztnfv8d') })\nwindow.addEventListener('load', function () { styleCell_tinytable_9gs4ubkyn36sjtay2wq6(1, 2, 'tinytable_css_4l8vlgkxxw42fztnfv8d') })\nwindow.addEventListener('load', function () { styleCell_tinytable_9gs4ubkyn36sjtay2wq6(2, 2, 'tinytable_css_4l8vlgkxxw42fztnfv8d') })\nwindow.addEventListener('load', function () { styleCell_tinytable_9gs4ubkyn36sjtay2wq6(3, 2, 'tinytable_css_4l8vlgkxxw42fztnfv8d') })\nwindow.addEventListener('load', function () { styleCell_tinytable_9gs4ubkyn36sjtay2wq6(4, 2, 'tinytable_css_4l8vlgkxxw42fztnfv8d') })\nwindow.addEventListener('load', function () { styleCell_tinytable_9gs4ubkyn36sjtay2wq6(5, 2, 'tinytable_css_4l8vlgkxxw42fztnfv8d') })\nwindow.addEventListener('load', function () { styleCell_tinytable_9gs4ubkyn36sjtay2wq6(6, 2, 'tinytable_css_4l8vlgkxxw42fztnfv8d') })\nwindow.addEventListener('load', function () { styleCell_tinytable_9gs4ubkyn36sjtay2wq6(7, 2, 'tinytable_css_4l8vlgkxxw42fztnfv8d') })\nwindow.addEventListener('load', function () { styleCell_tinytable_9gs4ubkyn36sjtay2wq6(8, 2, 'tinytable_css_4l8vlgkxxw42fztnfv8d') })\nwindow.addEventListener('load', function () { styleCell_tinytable_9gs4ubkyn36sjtay2wq6(9, 2, 'tinytable_css_4l8vlgkxxw42fztnfv8d') })\nwindow.addEventListener('load', function () { styleCell_tinytable_9gs4ubkyn36sjtay2wq6(10, 2, 'tinytable_css_4l8vlgkxxw42fztnfv8d') })\nwindow.addEventListener('load', function () { styleCell_tinytable_9gs4ubkyn36sjtay2wq6(11, 2, 'tinytable_css_4l8vlgkxxw42fztnfv8d') })\nwindow.addEventListener('load', function () { styleCell_tinytable_9gs4ubkyn36sjtay2wq6(8, 0, 'tinytable_css_q3g1zoy7myqw2jgelyvs') })\nwindow.addEventListener('load', function () { styleCell_tinytable_9gs4ubkyn36sjtay2wq6(8, 1, 'tinytable_css_q3g1zoy7myqw2jgelyvs') })\nwindow.addEventListener('load', function () { styleCell_tinytable_9gs4ubkyn36sjtay2wq6(8, 2, 'tinytable_css_q3g1zoy7myqw2jgelyvs') })\n    </script>\n\n  </body>\n\n</html>\n```\n\n:::\n:::\n\n\nWhy are the two estimates of $\\tau$ - $\\hat{\\beta}_1^{OLS}$ and $\\hat{\\beta}_2^{OLS}$ - not equal to 0.02? The OLS estimator is a random variable that has a distribution. The unconfoundedness of the treatment assignment and the homogeneous treatment effects mean that in expectation these estimators are unbiased and equal to 0.02. However, a particular realization may not be. In fact, the probability that $\\hat{\\beta}^{OLS}=\\tau_{ATE}=0.2$ is 0. \n\n### Monte Carlo simulation\n\nTo examine this characteristic of both estimators we will repeat the above process R-times. This will allow us to observe the overall distribution of the estimator. To do so we will write a short program that executes the same process R-times. The programme stores value of the estimator in a matrix. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nmat1 <- replicate(1000, {\n  df<-data.frame(matrix(ncol = 0, nrow = 200));\n  df$female <- rbinom(200,1,0.5);\n  df$age <- floor(46*runif(200) + 20);\n  df$error <- rnorm(200,0,0.55);\n  df$Y0 <- alpha + gamma_f*df$female + gamma_a*df$age + df$error;\n  df$temp <- runif(200);\n  df$treat <- ifelse(df$temp>=median(df$temp),1,0);\n  df$Yobs <- df$Y0 + tau*df$treat;\n  lm1 <- lm(Yobs ~ treat, data=df);\n  lm2 <- lm(Yobs ~ treat + female + age, data=df);\n  coef <- c(lm1$coefficients[2],lm2$coefficients[2])\n}, simplify = \"array\")\nmat2 <-t(mat1)\ncolnames(mat2)<-c(\"beta1\",\"beta2\")\nsummary(mat2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     beta1              beta2         \n Min.   :-0.50429   Min.   :-0.22257  \n 1st Qu.:-0.09083   1st Qu.:-0.03110  \n Median : 0.02168   Median : 0.01811  \n Mean   : 0.01898   Mean   : 0.02053  \n 3rd Qu.: 0.12914   3rd Qu.: 0.07469  \n Max.   : 0.52228   Max.   : 0.28544  \n```\n\n\n:::\n\n```{.r .cell-code}\ndescribe(mat2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      vars    n mean   sd median trimmed  mad   min  max range  skew kurtosis\nbeta1    1 1000 0.02 0.17   0.02    0.02 0.16 -0.50 0.52  1.03 -0.07     0.06\nbeta2    2 1000 0.02 0.08   0.02    0.02 0.08 -0.22 0.29  0.51 -0.02    -0.01\n        se\nbeta1 0.01\nbeta2 0.00\n```\n\n\n:::\n:::\n\n\nHaving repeated the process 1000 times we can now examine the characteristics of each estimator, first by summarizing the stored values and then graphically. First, using a histogram:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(as.data.frame(mat2), aes(x=beta1)) +\n  geom_histogram(colour=\"black\", fill=\"lightgrey\") +\n  geom_vline(aes(xintercept=mean(beta1)),\n            color=\"red\", linetype=\"dashed\", size=1)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](seminar-1_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n\n```{.r .cell-code}\nggplot(as.data.frame(mat2), aes(x=beta2)) +\n  geom_histogram(colour=\"black\", fill=\"lightgrey\") +\n  geom_vline(aes(xintercept=mean(beta2)),\n            color=\"red\", linetype=\"dashed\", size=1)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](seminar-1_files/figure-html/unnamed-chunk-11-2.png){width=672}\n:::\n:::\n\n\nSecond, using a kernel density function, which generates a smoothed estimate of the density function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(as.data.frame(mat2), aes(x=beta1)) +\n  geom_density(aes(x=beta1,color=\"Beta1\")) +\n  geom_density(aes(x=beta2,color=\"Beta2\"),linetype=\"dashed\") +\n  geom_vline(aes(xintercept=0.02,color=\"tau=0.02\")) +\n  scale_color_manual(\"\",\n                     breaks=c(\"Beta1\",\"Beta2\",\"tau=0.02\"),\n                     values=c(\"Beta1\"=\"red\", \"Beta2\"=\"blue\",\"tau=0.02\"=\"darkgreen\")) +\n  xlab(\"beta/tau\") +\n  labs(title=\"kernel density plot\")\n```\n\n::: {.cell-output-display}\n![](seminar-1_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\nThe distribution of both estimators is remarkably \\textit{normal}. This is by design. The distribution of $\\hat{\\beta}$ is dependent on the distribution of the error term. In this instance, that is a normal distribution because of the DGP we specified. \n\n### Power function\n\nTo compute the power function we need to know the variance of each of our two OLS estimators. Given that the variance of the estimator is in fact known, we could include this directly. Alternatively, we could estimate the variance of the each estimator using the distribution from the simulation. \n\nLet us see how close the simulation variance is to the true DGP variance. From the DGP, we know that\n\n$$\n\\begin{aligned}\n\tVar(\\hat{\\beta}^{OLS}_1) &= \\frac{\\gamma_f^2Var(female_i)+\\gamma_a^2Var(age_i)+\\sigma_{\\varepsilon}^2}{N(\\bar{W}-\\bar{W}^2)} \\\\\n\tVar(\\hat{\\beta}^{OLS}_2) &= \\frac{\\sigma_{\\varepsilon}^2}{N(\\bar{W}-\\bar{W}^2)}\n\\end{aligned}\n$$\nwhere,\n$$\n\t\\begin{aligned}\n\tVar(female_i) &= 0.5(1-0.5) \\\\\n\tVar(age_i) &= \\frac{1}{12}(65-20)^2\n\\end{aligned}\n$$\n\nCalculate standard error for each estimator based on known distributions.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nse1 <- sqrt((gamma_f^2*(0.5-0.5^2) + gamma_a^2*1/12*(65-20)^2 + 0.55^2)/(200*(0.5-0.5^2)))\nse2 <- sqrt((0.55^2)/(200*(0.5-0.5^2)))\nse1\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.1667168\n```\n\n\n:::\n\n```{.r .cell-code}\nse2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.07778175\n```\n\n\n:::\n:::\n\n\nApproximate the standard error using the standard deviation of the simulations.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsd1 <- sd(as.data.frame(mat2)$beta1)\nsd1\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.168046\n```\n\n\n:::\n\n```{.r .cell-code}\nsd2 <- sd(as.data.frame(mat2)$beta2)\nsd2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.07764108\n```\n\n\n:::\n:::\n\n\nFor now we will proceed to calculate the power function using the known DGP variance. To do so we will first construct a grid of alternative values of $beta$. Next, we will estimate the power for each true value of $beta$; i.e. each value in the grid. Finally, we will plot the two power functions alongside one another. \n\nConstruct grid of alternative values of $\\tau_0$, centered around $\\tau_0$=0.02. This should include $\\tau_0=0$, the null hypothesis.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata1$tau0[1]<- -0.1 + tau\nfor (i in 2:200) {\n  data1$tau0[i] <- data1$tau0[i-1] + 0.001\n}\n```\n:::\n\n\nCalculate power at each value of $\\tau_0$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata1$theta1 <- pnorm(qnorm(0.025)-data1$tau0/se1) + 1-pnorm(qnorm(0.975)-data1$tau0/se1)\ndata1$theta2 <- pnorm(qnorm(0.025)-data1$tau0/se2) + 1-pnorm(qnorm(0.975)-data1$tau0/se2)\n```\n:::\n\n\nPlot power curves\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data1, aes(x=tau0,y=theta1)) +\n  geom_line(aes(y=theta1,color=\"Beta1\")) +\n  geom_line(aes(y=theta2,color=\"Beta2\"),linetype=\"dashed\") +\n  geom_vline(aes(xintercept=0,color=\"H0\")) +\n  geom_vline(aes(xintercept=0.02,color=\"tau=0.02\")) +\n  geom_hline(aes(yintercept=0.05,color=\"a=0.05\")) +\n  scale_color_manual(\"\",\n                     breaks=c(\"Beta1\",\"Beta2\",\"H0\",\"tau=0.02\",\"a=0.05\"),\n                     values=c(\"Beta1\"=\"red\", \"Beta2\"=\"blue\",\"H0\"=\"black\",\n                              \"tau=0.02\"=\"darkgreen\",\"a=0.05\"=\"darkgrey\")) +\n  xlab(\"beta/tau\") +\n  ylab(\"power\") +\n  labs(title=\"power curves\")\n```\n\n::: {.cell-output-display}\n![](seminar-1_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n\nIt is clear from the displayed power functions that the second estimator is universally more powerful. The lower variance of this estimators translates directly into a higher probability of rejecting the null hypothesis of no-effect for all non-zero values of the true hypothesis. It is for this reason that we strongly prefer the second estimator and do not simply estimate a model of outcome on treatment. \n\t\nIn this instance, there is relatively little power to reject the null at the true value. Can you calculate the power at $\\tau_0=0.02$? \n\n### Sample size\n\nWe have explored the value of including covariates, but what about increasing the sample size? Repeat the above process with a sample size of $N=1000$ and compare the respective distributions and power functions of the estimators. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nmat3 <- replicate(1000, {\n  df2<-data.frame(matrix(ncol = 0, nrow = 1000));\n  df2$female <- rbinom(1000,1,0.5);\n  df2$age <- floor(46*runif(1000) + 20);\n  df2$error <- rnorm(1000,0,0.55);\n  df2$Y0 <- alpha + gamma_f*df2$female + gamma_a*df2$age + df2$error;\n  df2$temp <- runif(1000);\n  df2$treat <- ifelse(df2$temp>=median(df2$temp),1,0);\n  df2$Yobs <- df2$Y0 + tau*df2$treat;\n  lm1 <- lm(Yobs ~ treat, data=df2);\n  lm2 <- lm(Yobs ~ treat + female + age, data=df2);\n  coef <- c(lm1$coefficients[2],lm2$coefficients[2])\n}, simplify = \"array\")\nmat4 <-t(mat3)\ncolnames(mat4)<-c(\"beta1_1000\",\"beta2_1000\")\nsummary(mat4)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   beta1_1000         beta2_1000       \n Min.   :-0.16973   Min.   :-0.114858  \n 1st Qu.:-0.02846   1st Qu.:-0.004257  \n Median : 0.02295   Median : 0.020215  \n Mean   : 0.02234   Mean   : 0.019649  \n 3rd Qu.: 0.07392   3rd Qu.: 0.042947  \n Max.   : 0.26633   Max.   : 0.122476  \n```\n\n\n:::\n\n```{.r .cell-code}\ndescribe(mat4)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           vars    n mean   sd median trimmed  mad   min  max range  skew\nbeta1_1000    1 1000 0.02 0.08   0.02    0.02 0.08 -0.17 0.27  0.44  0.04\nbeta2_1000    2 1000 0.02 0.04   0.02    0.02 0.03 -0.11 0.12  0.24 -0.02\n           kurtosis se\nbeta1_1000    -0.26  0\nbeta2_1000     0.16  0\n```\n\n\n:::\n:::\n\n\nCalculate power for 1000 obs\n\n\n::: {.cell}\n\n```{.r .cell-code}\nse3 <- sqrt((gamma_f^2*(0.5-0.5^2) + gamma_a^2*1/12*(65-20)^2 + 0.55^2)/(1000*(0.5-0.5^2)))\nse4 <- sqrt((0.55^2)/(1000*(0.5-0.5^2)))\nse3\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.07455803\n```\n\n\n:::\n\n```{.r .cell-code}\nse4\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.03478505\n```\n\n\n:::\n\n```{.r .cell-code}\ndata1$theta3 <- pnorm(qnorm(0.025)-data1$tau0/se3) + 1-pnorm(qnorm(0.975)-data1$tau0/se3)\ndata1$theta4 <- pnorm(qnorm(0.025)-data1$tau0/se4) + 1-pnorm(qnorm(0.975)-data1$tau0/se4)\n```\n:::\n\n\nKernel Density (200 vs 1000)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmat5 <- cbind(mat2,mat4)\nggplot(as.data.frame(mat5), aes(x=beta1)) +\n  geom_density(aes(x=beta1,color=\"Beta1_200\")) +\n  geom_density(aes(x=beta2,color=\"Beta2_200\")) +\n  geom_vline(aes(xintercept=0.02,color=\"tau=0.02\")) +\n  geom_density(aes(x=beta1_1000,color=\"Beta1_1000\")) + \n  geom_density(aes(x=beta2_1000,color=\"Beta2_1000\")) + \n  scale_color_manual(\"\",\n                     breaks=c(\"Beta1_200\",\"Beta2_200\",\"Beta1_1000\",\"Beta2_1000\",\"tau=0.02\"),\n                     values=c(\"Beta1_200\"=\"red\", \"Beta2_200\"=\"blue\",\"tau=0.02\"=\"darkgreen\",\n                              \"Beta1_1000\"=\"orange\",\"Beta2_1000\"=\"purple\")) +\n  xlab(\"beta/tau\") +\n  labs(title=\"kernel density plot\")\n```\n\n::: {.cell-output-display}\n![](seminar-1_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n:::\n\n\nPlot power curves (200 vs 1000)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data1, aes(x=tau0,y=theta1)) +\n  geom_line(aes(y=theta1,color=\"theta1_200\")) +\n  geom_line(aes(y=theta2,color=\"theta2_200\")) +\n  geom_line(aes(y=theta3,color=\"theta1_1000\")) +\n  geom_line(aes(y=theta4,color=\"theta2_1000\")) +\n  geom_vline(aes(xintercept=0,color=\"H0\")) +\n  geom_vline(aes(xintercept=0.02,color=\"tau=0.02\")) +\n  geom_hline(aes(yintercept=0.05,color=\"a=0.05\")) +\n  scale_color_manual(\"\",\n                     breaks=c(\"theta1_200\",\"theta2_200\",\"theta1_1000\",\n                              \"theta2_1000\",\"H0\",\"tau=0.02\",\"a=0.05\"),\n                     values=c(\"theta1_200\"=\"red\",\"theta2_200\"=\"blue\",\n                              \"theta1_1000\"=\"orange\",\"theta2_1000\"=\"purple\",\n                              \"H0\"=\"black\",\"tau=0.02\"=\"darkgreen\",\"a=0.05\"=\"darkgrey\")) +\n  xlab(\"beta/tau\") +\n  ylab(\"power\") +\n  labs(title=\"power curves\")\n```\n\n::: {.cell-output-display}\n![](seminar-1_files/figure-html/unnamed-chunk-21-1.png){width=672}\n:::\n:::\n",
    "supporting": [
      "seminar-1_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}