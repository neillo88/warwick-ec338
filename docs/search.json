[
  {
    "objectID": "seminar-3.html",
    "href": "seminar-3.html",
    "title": "Seminar 3: Difference-in-Differences",
    "section": "",
    "text": "library(tidyverse)\nlibrary(modelsummary)\nlibrary(psych)\nlibrary(plm)\nlibrary(broom)"
  },
  {
    "objectID": "seminar-3.html#required-r-packages",
    "href": "seminar-3.html#required-r-packages",
    "title": "Seminar 3: Difference-in-Differences",
    "section": "",
    "text": "library(tidyverse)\nlibrary(modelsummary)\nlibrary(psych)\nlibrary(plm)\nlibrary(broom)"
  },
  {
    "objectID": "seminar-3.html#learning-objectives",
    "href": "seminar-3.html#learning-objectives",
    "title": "Seminar 3: Difference-in-Differences",
    "section": "2 Learning Objectives",
    "text": "2 Learning Objectives\nIn this seminar you will:\n\nestimate dynamic difference-in-differences models and plot coefficients (with standard errors);\nevaluate the significance of cohort and unit fixed-effects;\nobserve how estimators react to dynamic treatment effects, pre-emptive behaviour, and failure of parallel trends;\nand explore the impact of cohort heterogeneity and how to resolve it."
  },
  {
    "objectID": "seminar-3.html#motivation",
    "href": "seminar-3.html#motivation",
    "title": "Seminar 3: Difference-in-Differences",
    "section": "3 Motivation",
    "text": "3 Motivation\nA quick examination of the applied literature will reveal the wide range of (linear regression) model specifications used to estimate the average treatment effect of the treated (ATT) of a (natural) experiment. Beyond differences in notation, this variation exists for number of important reasons:\n\ndata: repeated cross-sections or panel data; more than two periods of observation;\nlevel of assignment: individual or group/cohort;\ntiming of assignment: simultaneous or staggered; absorbing or alternating; presence of a never-treated control;\nassumptions regarding the treatment effect: static or dynamic; homogeneous or heterogeneous\n\nFor example, consider a setting where you have multiple periods of data both before and after treatment and the treatment was assigned at simultaneously at the unit level. The following model specification, with both unit and time FEs,\n\\[\nY_{it} = \\alpha_i + \\delta_t + \\beta D_{i}\\cdot \\mathbf{1}\\{t\\geq t_0\\} + \\varepsilon_{it}\n\\] assumes that you have longitudinal data. You cannot estimate this model with repeated cross-sections, since each unit is observed once and the unit FEs will explain all the variation in \\(Y\\). With repeated cross-sections, you would have to estimate,\n\\[\nY_{it} = \\alpha + \\psi D_i + \\delta_t + \\beta D_{i}\\cdot \\mathbf{1}\\{t\\geq t_0\\} + \\varepsilon_{it}\n\\]\nMoreover, by specifying a model that estimates a single post-treatment coefficient, you are implicitly making certain assumptions about the dynamics of the ATT. Afterall, if you believed that the treatment effect varied over time, why would you need estimate a dynamic specification; for example,\n\\[\nY_{it} = \\alpha_i + \\delta_t + \\sum_{j\\neq t_0-1} \\beta_j D_{i}\\cdot \\mathbf{1}\\{t-t_0=j\\} + \\varepsilon_{it}\n\\]\nThe dynamic specification would allow you to test for parallel trends or anticipatory behaviour.\nIn this seminar we will explore how to match the model specification to the experimental and empirical setting as well as the implications of getting this wrong."
  },
  {
    "objectID": "seminar-3.html#setup",
    "href": "seminar-3.html#setup",
    "title": "Seminar 3: Difference-in-Differences",
    "section": "4 Setup",
    "text": "4 Setup\nWe will simulate a panel (longitudinal) dataset in long-form. In long-form, each row-observation in the dataset represents a unit ($i$) in a single time period (\\(t\\)). There will be \\(t\\) row-observations of each unit \\(i\\) in a balanced panel. Panel data can also be arranged in wide-form, where each unit appears once and repeated observations are arranged as separate columns. To estimate a linear regression model, the data must be in long-form.1\nStart by setting a seed. This ensures that all random number generators in this programme are replicable.\n\nset.seed(27310)\n\n\nn &lt;- 2000\nt &lt;- 11\n\n\n4.1 Generate data\nGenerate an empty dataframe with 1000 observations.\n\ndf &lt;- data.frame(matrix(ncol = 0, nrow = n))\n\nCreate an ID variable equal to the row number.\n\ndf$id &lt;- 1:nrow(df)\n\nSplit ID’s into different sized treatment groups.\n\ndf$group &lt;- cut(df$id,\n                 breaks=c(0,500,1000,1400,1700,1900,2000),\n                 right=TRUE\n)\ndf$group &lt;- as.integer(df$group)\n\nThe last step converts the categorical variable into an integer.\nDuplicate the ID’s for each period.\n\n  data &lt;- df\n  i &lt;- 1\n  while (i&lt;t) {\n    data &lt;- rbind(data,df)\n    i &lt;- i+1\n  }\n\nGenerate time variable.\n\ndata &lt;- data %&gt;%\n  group_by(id) %&gt;%\n  mutate(time = 1:t) %&gt;%\n  arrange(id,time)\n\nWe are going to generate the underlying \\(Y(0)\\) variable based on a two-way FEs structure:\n\\[\n  Y_{it}(0) = \\alpha_i + \\delta_t + \\varepsilon_{it}\n\\] with,\n\n\\(\\alpha = 1+0.05\\cdot (c-1)\\) where \\(c\\) is a individual’s cohort. This ensures level differences across treatment groups.\n\\(\\delta_t = 1 +0.1\\cdot (t-1)\\), which will generate a linear time trend.\n\\(\\varepsilon_{it}\\sim_{iid} \\; N(0,\\sigma)\\)\n\nSet values\n\nsigma &lt;- 0.55\ndata &lt;- data %&gt;%\n  mutate(\n    alpha = 1 + 0.05*group,\n    delta = 1 + 0.1*time,\n    epsilon = rnorm(n(),0,sigma),\n    y0 = alpha + delta + epsilon\n    )\n\nThe data we have generated has parallel trends across groups. We demonstrate this fact by plotting the average of the outcome for each group, over time.\n\n# Compute group averages\ndata_avg &lt;- data %&gt;%\n  group_by(time, group) %&gt;%\n  summarize(avg_y0 = mean(y0), .groups = 'drop')\n\n# Plot the time trend of group averages\nggplot(data_avg, aes(x = time, y = avg_y0, color = group, group = group)) +\n  geom_line() +                      # Add lines for each group\n  geom_point() +                     # Add points to show averages\n  labs(\n    title = \"Time trend of Y(0) by group\",\n    x = \"Time\",\n    y = \"E[Y(0)]\",\n    color = \"Group\"\n  )"
  },
  {
    "objectID": "seminar-3.html#case-1-dynamic-difference-in-difference",
    "href": "seminar-3.html#case-1-dynamic-difference-in-difference",
    "title": "Seminar 3: Difference-in-Differences",
    "section": "5 Case 1: Dynamic Difference-in-Difference",
    "text": "5 Case 1: Dynamic Difference-in-Difference\nWe begin by examining the instance where half of the sample are treated in period 6 (groups 1 and 2), while the remaining sample is never treated.\n\nLet’s create the following three dummy variables:\n\n\n\\(D_i = \\mathbf{1}\\{c\\leq 2\\}\\): indicator for treated group (time-invariant)\n\\(T_t = \\mathbf{1}\\{t&gt;=6\\}\\): indicator for post treatment periods\n\\(D_i\\cdot T_t\\): indicator for treated group, after treatment (time-varying)\n\n\ndata1 &lt;- data\ndata1 &lt;- data1 %&gt;%\n  mutate(trtgrp = (group&lt;=2)*1,\n         post = (time&gt;=6)*1,\n         trtpost = trtgrp*post, \n         trttime = as.factor(trtgrp*time),\n         trttimepost = as.factor(trtgrp*time*post))\n\nFirst, we will generate data based on a static, homogeneous treatment effect: \\(\\tau = 0.2\\).\n\ndata1 &lt;- data1 %&gt;%\n  mutate(yobs = y0 + trtpost*0.1)\n\nGiven this DGP, there are a few specifications we can estimate.\n\nSimple (i.e. 2-group-2-period) specification \\[\nY_{it} = \\alpha + \\psi D_i + \\delta T_t + \\beta D_{i}\\cdot T_t + \\varepsilon_{it}\n\\]\nStatic specification with time FEs, but no unit FEs \\[\nY_{it} = \\alpha + \\psi D_i + \\delta_t + \\beta D_{i}\\cdot T_t + \\varepsilon_{it}\n\\]\nStatic specification with two-way FEs \\[\nY_{it} = \\alpha_i + \\delta_t + \\beta D_{i}\\cdot T_t + \\varepsilon_{it}\n\\]\nSemi-dynamic specification (with group or unit FEs) \\[\nY_{it} = \\alpha_i + \\delta_t + \\sum_{j&gt;5} \\beta_j D_{i}\\cdot \\mathbf{1}\\{t-6=j\\} + \\varepsilon_{it}\n\\]\nDynamic specification (with group or unit FEs) \\[\nY_{it} = \\alpha_i + \\delta_t + \\sum_{j\\neq 5} \\beta_j D_{i}\\cdot \\mathbf{1}\\{t-6=j\\} + \\varepsilon_{it}\n\\]\n\n\nreg1 &lt;- lm(yobs ~ trtpost + trtgrp + post, data=data1)\n\n\nreg2 &lt;- plm(yobs ~ trtpost + trtgrp, data=data1, index=c(\"id\",\"time\"), model = \"within\", effect = \"time\")\n\n# Alternative programming (computes different R2):\n#reg2 &lt;- lm(yobs ~ trtpost + trtgrp + as.factor(time), data=data1)\n\nreg3 &lt;- plm(yobs ~ trtpost, data=data1, index=c(\"id\",\"time\"), model = \"within\", effect = \"twoways\")\n\nmodelsummary(list(\"(1)\"=reg1,\"(2)\"=reg2, \"(3)\"=reg3), stars=c('*'=.1, '**'=.05,'***'=.01), coef_omit=-(2), gof_map = c(\"nobs\", \"r.squared\", \"rmse\"), title = \"Static specifications 1-3\")\n\n \n\n  \n    \n    \n    tinytable_mu0z9icyxrf9w843n5s3\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n        Static specifications 1-3\n              \n                 \n                (1)\n                (2)\n                (3)\n              \n        \n        * p &lt; 0.1, ** p &lt; 0.05, *** p &lt; 0.01\n        \n                \n                  trtpost \n                  0.106***\n                  0.106***\n                  0.106***\n                \n                \n                          \n                  (0.016) \n                  (0.015) \n                  (0.015) \n                \n                \n                  Num.Obs.\n                  22000   \n                  22000   \n                  22000   \n                \n                \n                  R2      \n                  0.221   \n                  0.006   \n                  0.003   \n                \n                \n                  RMSE    \n                  0.57    \n                  0.55    \n                  0.52    \n                \n        \n      \n    \n\n    \n\n  \n\n\n\n\nThe dynamic specifications,\n\ndata1 &lt;- data1 %&gt;%\n  mutate(btrtgrp = as.factor(trtgrp),\n         btrttime = as.factor(trttime),\n         btrttime = relevel(trttime,\"5\"),\n         btrttimepost = as.factor(trttimepost)\n  )\n\nreg4a &lt;- plm(yobs ~ btrtgrp + btrttimepost, data=data1, index=c(\"id\",\"time\"), model = \"within\", effect = \"time\")\n\nreg4b &lt;- plm(yobs ~ btrttimepost, data=data1, index=c(\"id\",\"time\"), model = \"within\", effect = \"twoways\")\n\nmodelsummary(list(\"(4a)\"=reg4a,\"(4b)\"=reg4b), stars=c('*'=.1, '**'=.05,'***'=.01), coef_omit=(\"btrtgrp\"), gof_map = c(\"nobs\", \"r.squared\", \"rmse\"), title = \"Semi-dynamic specification 4\")\n\n \n\n  \n    \n    \n    tinytable_d3bplbq7brbnvm7caoe4\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n        Semi-dynamic specification 4\n              \n                 \n                (4a)\n                (4b)\n              \n        \n        * p &lt; 0.1, ** p &lt; 0.05, *** p &lt; 0.01\n        \n                \n                  btrttimepost6 \n                  0.101***\n                  0.101***\n                \n                \n                                \n                  (0.027) \n                  (0.027) \n                \n                \n                  btrttimepost7 \n                  0.132***\n                  0.132***\n                \n                \n                                \n                  (0.027) \n                  (0.027) \n                \n                \n                  btrttimepost8 \n                  0.121***\n                  0.121***\n                \n                \n                                \n                  (0.027) \n                  (0.027) \n                \n                \n                  btrttimepost9 \n                  0.094***\n                  0.094***\n                \n                \n                                \n                  (0.027) \n                  (0.027) \n                \n                \n                  btrttimepost10\n                  0.104***\n                  0.104***\n                \n                \n                                \n                  (0.027) \n                  (0.027) \n                \n                \n                  btrttimepost11\n                  0.087***\n                  0.087***\n                \n                \n                                \n                  (0.027) \n                  (0.027) \n                \n                \n                  Num.Obs.      \n                  22000   \n                  22000   \n                \n                \n                  R2            \n                  0.006   \n                  0.003   \n                \n                \n                  RMSE          \n                  0.55    \n                  0.52    \n                \n        \n      \n    \n\n    \n\n  \n\n\n\nreg5a &lt;- plm(yobs ~ btrtgrp + btrttime, data=data1, index=c(\"id\",\"time\"), model = \"within\", effect = \"time\")\n\nreg5b &lt;- plm(yobs ~ btrttime, data=data1, index=c(\"id\",\"time\"), model = \"within\", effect = \"twoways\")\n\nmodelsummary(list(\"(5a)\"=reg5a, \"(5b)\"=reg5b), stars=c('*'=.1, '**'=.05,'***'=.01), coef_omit=(\"btrtgrp\"), gof_map = c(\"nobs\", \"r.squared\", \"rmse\"), title = \"Dynamic specification 5\")\n\n \n\n  \n    \n    \n    tinytable_gtxsi83lvl5hny7q4lqz\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n        Dynamic specification 5\n              \n                 \n                (5a)\n                (5b)\n              \n        \n        * p &lt; 0.1, ** p &lt; 0.05, *** p &lt; 0.01\n        \n                \n                  btrttime1 \n                  0.022   \n                  0.022   \n                \n                \n                            \n                  (0.035) \n                  (0.035) \n                \n                \n                  btrttime2 \n                  0.022   \n                  0.022   \n                \n                \n                            \n                  (0.035) \n                  (0.035) \n                \n                \n                  btrttime3 \n                  0.009   \n                  0.009   \n                \n                \n                            \n                  (0.035) \n                  (0.035) \n                \n                \n                  btrttime4 \n                  0.002   \n                  0.002   \n                \n                \n                            \n                  (0.035) \n                  (0.035) \n                \n                \n                  btrttime6 \n                  0.112***\n                  0.112***\n                \n                \n                            \n                  (0.035) \n                  (0.035) \n                \n                \n                  btrttime7 \n                  0.143***\n                  0.143***\n                \n                \n                            \n                  (0.035) \n                  (0.035) \n                \n                \n                  btrttime8 \n                  0.132***\n                  0.132***\n                \n                \n                            \n                  (0.035) \n                  (0.035) \n                \n                \n                  btrttime9 \n                  0.105***\n                  0.105***\n                \n                \n                            \n                  (0.035) \n                  (0.035) \n                \n                \n                  btrttime10\n                  0.115***\n                  0.115***\n                \n                \n                            \n                  (0.035) \n                  (0.035) \n                \n                \n                  btrttime11\n                  0.098***\n                  0.098***\n                \n                \n                            \n                  (0.035) \n                  (0.035) \n                \n                \n                  Num.Obs.  \n                  22000   \n                  22000   \n                \n                \n                  R2        \n                  0.006   \n                  0.003   \n                \n                \n                  RMSE      \n                  0.55    \n                  0.52    \n                \n        \n      \n    \n\n    \n\n  \n\n\n\n\nHowever, with dynamic models it makes more sense to plot the coefficients in a graph. Let us quick create a function that will graph the plots.\n\neventplot &lt;- function(model,start,end,treat) {\n  coef_model &lt;- tidy(model, conf.int = TRUE)\n  # extract event-times from coefficient labels\n  coef_model$time &lt;- as.integer(substr(coef_model$term,9,10))\n  #append row with estimate=0 for base period\n  coef_0 &lt;- data.frame(term = NA, estimate = 0, std.error = NA, statistic = NA, p.value = NA, conf.low = NA, conf.high = NA, time = -1)\n  coef_model &lt;- rbind(coef_model,coef_0)\n  coef_model &lt;- arrange(coef_model,time)\n  # plot graph\n  graph &lt;- ggplot(coef_model, aes(x = time, y = estimate)) +\n    geom_point() +            # Plot the point estimates\n    geom_line() +             # Connect point estimates with line\n    geom_line(y=0) +          # Horizontal line\n    geom_vline(xintercept = treat-0.5, color = \"red\", linetype = \"dashed\", size = 1) +        \n    geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.2) +  # Add error bars for the confidence intervals\n    labs(\n      title = \"Dynamic coefficients from specification 5\",\n      x = \"Event-time\",\n      y = \"Estimate\"\n    ) +\n    scale_x_continuous(breaks = -start:end, labels = -start:end)\n  return(graph)\n}\n\nLet’s try this specification 5, with unit FEs.\n\neventplot(reg5b,1,11,6)\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\nFigure 1: test"
  },
  {
    "objectID": "seminar-3.html#case-2-staggered-intervention",
    "href": "seminar-3.html#case-2-staggered-intervention",
    "title": "Seminar 3: Difference-in-Differences",
    "section": "6 Case 2: Staggered Intervention",
    "text": "6 Case 2: Staggered Intervention\nNext, we will examine the staggered intervention case. Group 1 will remain untreated, while groups 2-6 are treated at separated times. We will use the groups as separate cohorts with treatment times given by,\n\\[\nS_c \\in \\{\\infty,4,5,6,7,8\\}\n\\]\nWe begin by assigning each of the groups in the simulated dataset a cohort value, as above.\n\ndata2 &lt;- data %&gt;%\n  mutate(cohort = ifelse(group&gt;1,group+2,NA),\n         etime = time-cohort,\n         )\ntable(data2$etime, data2$cohort, useNA = \"ifany\")\n\n      \n          4    5    6    7    8 &lt;NA&gt;\n  -7      0    0    0    0  100    0\n  -6      0    0    0  200  100    0\n  -5      0    0  300  200  100    0\n  -4      0  400  300  200  100    0\n  -3    500  400  300  200  100    0\n  -2    500  400  300  200  100    0\n  -1    500  400  300  200  100    0\n  0     500  400  300  200  100    0\n  1     500  400  300  200  100    0\n  2     500  400  300  200  100    0\n  3     500  400  300  200  100    0\n  4     500  400  300  200    0    0\n  5     500  400  300    0    0    0\n  6     500  400    0    0    0    0\n  7     500    0    0    0    0    0\n  &lt;NA&gt;    0    0    0    0    0 5500\n\n\nNote, these two variables have missing values corresponding to the never-treated group. From a programming perspective we do not want any variables in the estimator to have missing values. For this reason, we will create a set of event-time variables that will be used in the estimator that have no missing values. We will do so by replacing NT treated value of event-time to the chosen based period.\n\nbase &lt;- -1\n\ndata2$trtgrp &lt;- data2$cohort\ndata2$trtgrp[is.na(data2$trtgrp)] &lt;- base\n# cross-tabulate treatment-group and cohort\ntable(data2$trtgrp, data2$cohort, useNA = \"ifany\")\n\n    \n        4    5    6    7    8 &lt;NA&gt;\n  -1    0    0    0    0    0 5500\n  4  5500    0    0    0    0    0\n  5     0 4400    0    0    0    0\n  6     0    0 3300    0    0    0\n  7     0    0    0 2200    0    0\n  8     0    0    0    0 1100    0\n\ndata2$trttime &lt;- data2$etime\ndata2$trttime[is.na(data2$trttime)] &lt;- base \n# cross-tabulate event-time and cohort\ntable(data2$trttime, data2$cohort, useNA = \"ifany\")\n\n    \n        4    5    6    7    8 &lt;NA&gt;\n  -7    0    0    0    0  100    0\n  -6    0    0    0  200  100    0\n  -5    0    0  300  200  100    0\n  -4    0  400  300  200  100    0\n  -3  500  400  300  200  100    0\n  -2  500  400  300  200  100    0\n  -1  500  400  300  200  100 5500\n  0   500  400  300  200  100    0\n  1   500  400  300  200  100    0\n  2   500  400  300  200  100    0\n  3   500  400  300  200  100    0\n  4   500  400  300  200    0    0\n  5   500  400  300    0    0    0\n  6   500  400    0    0    0    0\n  7   500    0    0    0    0    0\n\ndata2$trttimepost = data2$trttime\ndata2$trttimepost[(data2$trttimepost&lt;0)] &lt;- base\n# cross-tabulate event-time and post-treatment-event-times\ntable(data2$trttimepost, data2$etime, useNA = \"ifany\")\n\n    \n       -7   -6   -5   -4   -3   -2   -1    0    1    2    3    4    5    6    7\n  -1  100  300  600 1000 1500 1500 1500    0    0    0    0    0    0    0    0\n  0     0    0    0    0    0    0    0 1500    0    0    0    0    0    0    0\n  1     0    0    0    0    0    0    0    0 1500    0    0    0    0    0    0\n  2     0    0    0    0    0    0    0    0    0 1500    0    0    0    0    0\n  3     0    0    0    0    0    0    0    0    0    0 1500    0    0    0    0\n  4     0    0    0    0    0    0    0    0    0    0    0 1400    0    0    0\n  5     0    0    0    0    0    0    0    0    0    0    0    0 1200    0    0\n  6     0    0    0    0    0    0    0    0    0    0    0    0    0  900    0\n  7     0    0    0    0    0    0    0    0    0    0    0    0    0    0  500\n    \n     &lt;NA&gt;\n  -1 5500\n  0     0\n  1     0\n  2     0\n  3     0\n  4     0\n  5     0\n  6     0\n  7     0\n\n\nUsing the same underlying two-way FEs DGP, we generate a new dataset that includes a dynamic treatment effect.\n\\[\n  Y^{obs}_{it} = \\alpha_i + \\delta_t + \\sum_{s}\\tau(s)\\mathbf{1}\\{t-S_i=s\\} + \\varepsilon_{it}\n\\] We will allow the treatment effect to increase with (event) time, but remain homogeneous across units/cohorts: \\(\\tau(s) = 0.1 + 0.025\\cdot s\\) for \\(s\\geq 0\\); where \\(s\\) is event-time. Note, we have ruled out anticipation.\n\ndata2 &lt;- data2 %&gt;%\n  mutate(yobs1 = y0 + (0.1 + 0.025*trttime)*(trttime&gt;=0)\n         )\n\nThe dynamic specifications,\n\n# create factor variables to use in plm() and set base category\ndata2 &lt;- data2 %&gt;%\n  mutate(btrtgrp = as.factor(trtgrp),\n         btrtgrp = relevel(btrtgrp,ref = \"-1\"),\n         btrttime = relevel(as.factor(trttime), \"-1\"),\n         btrttimepost = relevel(as.factor(trttimepost),\"-1\")\n  )\n\nreg6a &lt;- plm(yobs1 ~ btrtgrp + btrttimepost, data=data2, index=c(\"id\",\"time\"), model = \"within\", effect = \"time\")\n\nreg6b &lt;- plm(yobs1 ~ btrttimepost, data=data2, index=c(\"id\",\"time\"), model = \"within\", effect = \"twoways\")\n\nmodelsummary(list(\"6(a)\"=reg6a,\"6(b)\"=reg6b), stars=c('*'=.1, '**'=.05,'***'=.01), gof_map = c(\"nobs\", \"r.squared\", \"rmse\"), title = \"Semi-dynamic specifications\")\n\n \n\n  \n    \n    \n    tinytable_ra6e8kmuewkjshcjzkr9\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n        Semi-dynamic specifications\n              \n                 \n                6(a)\n                6(b)\n              \n        \n        * p &lt; 0.1, ** p &lt; 0.05, *** p &lt; 0.01\n        \n                \n                  btrtgrp4     \n                  0.065***\n                          \n                \n                \n                               \n                  (0.016) \n                          \n                \n                \n                  btrtgrp5     \n                  0.111***\n                          \n                \n                \n                               \n                  (0.015) \n                          \n                \n                \n                  btrtgrp6     \n                  0.172***\n                          \n                \n                \n                               \n                  (0.015) \n                          \n                \n                \n                  btrtgrp7     \n                  0.192***\n                          \n                \n                \n                               \n                  (0.015) \n                          \n                \n                \n                  btrtgrp8     \n                  0.230***\n                          \n                \n                \n                               \n                  (0.019) \n                          \n                \n                \n                  btrttimepost0\n                  0.083***\n                  0.083***\n                \n                \n                               \n                  (0.018) \n                  (0.018) \n                \n                \n                  btrttimepost1\n                  0.105***\n                  0.105***\n                \n                \n                               \n                  (0.019) \n                  (0.019) \n                \n                \n                  btrttimepost2\n                  0.120***\n                  0.120***\n                \n                \n                               \n                  (0.020) \n                  (0.020) \n                \n                \n                  btrttimepost3\n                  0.151***\n                  0.151***\n                \n                \n                               \n                  (0.021) \n                  (0.021) \n                \n                \n                  btrttimepost4\n                  0.212***\n                  0.212***\n                \n                \n                               \n                  (0.022) \n                  (0.022) \n                \n                \n                  btrttimepost5\n                  0.197***\n                  0.197***\n                \n                \n                               \n                  (0.025) \n                  (0.025) \n                \n                \n                  btrttimepost6\n                  0.243***\n                  0.243***\n                \n                \n                               \n                  (0.028) \n                  (0.028) \n                \n                \n                  btrttimepost7\n                  0.262***\n                  0.262***\n                \n                \n                               \n                  (0.036) \n                  (0.036) \n                \n                \n                  Num.Obs.     \n                  22000   \n                  22000   \n                \n                \n                  R2           \n                  0.037   \n                  0.006   \n                \n                \n                  RMSE         \n                  0.55    \n                  0.52    \n                \n        \n      \n    \n\n    \n\n  \n\n\n\nreg7a &lt;- plm(yobs1 ~ btrtgrp + btrttime, data=data2, index=c(\"id\",\"time\"), model = \"within\", effect = \"time\")\n\nreg7b &lt;- plm(yobs1 ~ btrttime, data=data2, index=c(\"id\",\"time\"), model = \"within\", effect = \"twoways\")\n\nmodelsummary(list(\"7(a)\"=reg7a, \"7(b)\"=reg7b), stars=c('*'=.1, '**'=.05,'***'=.01), gof_map = c(\"nobs\", \"r.squared\", \"rmse\"), title = \"Dynamic specifications\")\n\n \n\n  \n    \n    \n    tinytable_t1dmxcwithjb5vctzbbk\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n        Dynamic specifications\n              \n                 \n                7(a)\n                7(b)\n              \n        \n        * p &lt; 0.1, ** p &lt; 0.05, *** p &lt; 0.01\n        \n                \n                  btrtgrp4  \n                  0.051***\n                          \n                \n                \n                            \n                  (0.019) \n                          \n                \n                \n                  btrtgrp5  \n                  0.098***\n                          \n                \n                \n                            \n                  (0.019) \n                          \n                \n                \n                  btrtgrp6  \n                  0.157***\n                          \n                \n                \n                            \n                  (0.019) \n                          \n                \n                \n                  btrtgrp7  \n                  0.177***\n                          \n                \n                \n                            \n                  (0.021) \n                          \n                \n                \n                  btrtgrp8  \n                  0.223***\n                          \n                \n                \n                            \n                  (0.025) \n                          \n                \n                \n                  btrttime-3\n                  0.013   \n                  0.013   \n                \n                \n                            \n                  (0.022) \n                  (0.022) \n                \n                \n                  btrttime-2\n                  0.030   \n                  0.030   \n                \n                \n                            \n                  (0.021) \n                  (0.021) \n                \n                \n                  btrttime0 \n                  0.096***\n                  0.096***\n                \n                \n                            \n                  (0.021) \n                  (0.021) \n                \n                \n                  btrttime1 \n                  0.118***\n                  0.118***\n                \n                \n                            \n                  (0.022) \n                  (0.022) \n                \n                \n                  btrttime2 \n                  0.133***\n                  0.133***\n                \n                \n                            \n                  (0.023) \n                  (0.023) \n                \n                \n                  btrttime3 \n                  0.163***\n                  0.163***\n                \n                \n                            \n                  (0.023) \n                  (0.023) \n                \n                \n                  btrttime4 \n                  0.226***\n                  0.226***\n                \n                \n                            \n                  (0.025) \n                  (0.025) \n                \n                \n                  btrttime5 \n                  0.210***\n                  0.210***\n                \n                \n                            \n                  (0.027) \n                  (0.027) \n                \n                \n                  btrttime6 \n                  0.256***\n                  0.256***\n                \n                \n                            \n                  (0.030) \n                  (0.030) \n                \n                \n                  btrttime7 \n                  0.276***\n                  0.276***\n                \n                \n                            \n                  (0.037) \n                  (0.037) \n                \n                \n                  btrttime-4\n                  0.005   \n                  0.005   \n                \n                \n                            \n                  (0.025) \n                  (0.025) \n                \n                \n                  btrttime-5\n                  0.041   \n                  0.041   \n                \n                \n                            \n                  (0.031) \n                  (0.031) \n                \n                \n                  btrttime-6\n                  0.010   \n                  0.010   \n                \n                \n                            \n                  (0.040) \n                  (0.040) \n                \n                \n                  btrttime-7\n                  -0.071  \n                  -0.071  \n                \n                \n                            \n                  (0.063) \n                  (0.063) \n                \n                \n                  Num.Obs.  \n                  22000   \n                  22000   \n                \n                \n                  R2        \n                  0.037   \n                  0.006   \n                \n                \n                  RMSE      \n                  0.55    \n                  0.52    \n                \n        \n      \n    \n\n    \n\n  \n\n\n\n\nWe can use the same function to plot the last model.\n\neventplot(reg7b,-7,7,0)"
  },
  {
    "objectID": "seminar-3.html#case-3-failure-of-parallel-trends-and-anticipation",
    "href": "seminar-3.html#case-3-failure-of-parallel-trends-and-anticipation",
    "title": "Seminar 3: Difference-in-Differences",
    "section": "7 Case 3: Failure of parallel trends and anticipation",
    "text": "7 Case 3: Failure of parallel trends and anticipation\nLet’s generate an alternative observed outcome where the units respond to the treatment two periods before treatment is assigned.\n\ndata2 &lt;- data2 %&gt;%\n  mutate(yobs2 = y0 + (0.1 + 0.025*(trttime+2))*(trttime+2&gt;=0)*(trtgrp&gt;0)\n         )\n# the last condition ensures that never-treated are excluded\n\nreg8 &lt;- plm(yobs2 ~ btrttime, data=data2, index=c(\"id\",\"time\"), model = \"within\", effect = \"twoways\")\n\neventplot(reg8,-7,7,0)\n\n\n\n\n\n\n\n\nAnd a second instance where parallel trends do not hold. Recall, the trend component of \\(Y(0)\\) was given by \\(\\delta_t = 1 +0.1\\cdot (t-1)\\). Let’s remove the trend for the never-treated group.\n\ndata2 &lt;- data2 %&gt;%\n  mutate(yobs3 = y0 -0.1*(time-1)*(trtgrp==-1) + (0.1 + 0.025*trttime)*(trttime&gt;=0)\n         )\n# the last condition ensures that never-treated are excluded\n\nreg9 &lt;- plm(yobs3 ~ btrttime, data=data2, index=c(\"id\",\"time\"), model = \"within\", effect = \"twoways\")\n\neventplot(reg9,-7,7,0)"
  },
  {
    "objectID": "seminar-3.html#case-4-cohort-specific-att",
    "href": "seminar-3.html#case-4-cohort-specific-att",
    "title": "Seminar 3: Difference-in-Differences",
    "section": "8 Case 4: Cohort-specific ATT",
    "text": "8 Case 4: Cohort-specific ATT\nINCOMPLETE"
  },
  {
    "objectID": "seminar-3.html#footnotes",
    "href": "seminar-3.html#footnotes",
    "title": "Seminar 3: Difference-in-Differences",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nYou can reshape data from wide to long (or vice versa) in STATA using the reshape command. In R this can be done using the reshape2 package.↩︎"
  },
  {
    "objectID": "material-linearalgebra.html",
    "href": "material-linearalgebra.html",
    "title": "Linear Algebra",
    "section": "",
    "text": "Consider a set of \\(k\\) \\(n\\)-dimensional vectors \\(\\{X_{1},X_{2},...,X_{k}\\}\\). These vectors are,\n\nDefinition 1 linearly dependent if there exists a set of scalars \\(\\{a_{1},a_{2},\\dots,a_{k}\\}\\) such that\n\\[\na_1X_1 + a_2X_2+\\ldots+a_kX_k=0\n\\]\nwhere at least one \\(a_i\\neq0\\).\n\nAlternatively, they are,\n\nDefinition 2 linearly independent if the only set of scalars \\(\\{a_{1},a_{2},\\dots,a_{k}\\}\\) that satisfies the above condition is \\(a_1,a_2,\\dots,a_k=0\\).\n\nIf we collect these \\(k\\) column-vectors in a matrix, \\(X=[X_1\\;X_2 \\dots X_k]\\), then the linear dependence condition can be written as,\n\\[\na_1X_1 + a_2X_2+\\ldots+a_kX_k=\\begin{bmatrix} X_1\\;X_2 \\dots X_k\\end{bmatrix}\\begin{bmatrix}a_1\\\\a_2\\\\\\vdots\\\\a_k\\end{bmatrix}=Xa = 0\n\\]\nGiven any \\(n\\times k\\) matrix \\(X\\), its columns are,\n\nDefinition 3 linearly dependent if there exists a vector \\(a\\in\\mathbb{R}^k\\) such that \\(a\\neq0\\) and \\(Xa=0\\);\n\nor,\n\nDefinition 4 linearly independent if the only vector \\(a\\in\\mathbb{R}^k\\) such that \\(Xa=0\\) is \\(a=0\\).\n\nFor any matrix there may be more than one vector \\(a\\in\\mathbb{R}^{k}\\) such that \\(Xa=0\\). Indeed, if both \\(a_{1},a_{2}\\in\\mathbb{R}^{k}\\)\nsatisfy this condition and \\(a_{1}\\neq a_{2}\\) then you can show that any linear combination of \\(\\{a_{1},a_{2}\\}\\) satisfies the\ncondition \\(X(a_{1}b_{1}+a_{2}b_{2})=0\\) for \\(b_{1},b_{2}\\in\\mathbb{R}\\). Thus, there exists an entire set of vectors which satisfy this condition. This set is referred to as the,\n\nDefinition 5 null space of \\(X\\), \\[\n\\mathcal{N}(X) = \\{a\\in\\mathbb{R}^k:\\;Xa=0\\}\n\\]\n\nIt should be evident from the definition that if the columns of \\(X\\) are linearly independent then \\(\\mathcal{N}(X)=\\{0\\}\\), a singleton. That is, it just includes the 0-vector.\n\n\n\nHere, we concern ourselves only with real vectors from \\(\\mathbb{R}^n\\).\n\nDefinition 6 A vector space, denoted \\(\\mathcal{V}\\), refers to a set of vectors which is closed under finite addition and scalar multiplication.\n\n\nDefinition 7 A set of \\(k\\) linearly independent vectors, \\(\\{X_1,X_2,\\dots,X_k\\}\\), forms a basis for vector space \\(\\mathcal{V}\\) if \\(\\forall\\;y\\in\\mathcal{V}\\) there exists a set of \\(k\\) scalars such that, \\[\ny=X_1b_1+X_2b_2+\\ldots+X_kb_k\n\\]\n\nBased on these definitions, it is evident that for the Euclidean space, \\(\\mathbb{E}^n\\), any \\(n\\) linearly independent vectors from \\(\\mathbb{R}^n\\) is a basis. For example, any point in \\(\\mathbb{E}^2\\) can be defined as a multiple of,\n\\[\n\\begin{bmatrix}1\\\\0\\end{bmatrix}\\quad \\text{and} \\quad\\begin{bmatrix}0\\\\1\\end{bmatrix}\n\\]\nConsider again the \\(n\\times k\\) matrix \\(X\\), where \\(k&lt;n\\). Then we define the,\n\nDefinition 8 column space (or span) of \\(X\\), denoted \\(\\mathcal{S}(X)\\), as the vector space generate by the \\(k\\) columns of \\(X\\). Formally, \\[\n\\mathcal{S}(X) = \\{y\\in\\mathbb{R}^n:\\;y=Xb\\quad\\text{for some }b\\in \\mathbb{R}^k\\}\n\\]\n\nA property to note about the span or column space \\(X\\) is,\nResult: \\(\\mathcal{S}(X)=\\mathcal{S}(XX')\\) :::\nwhere \\(XX'\\) is a \\(n\\times n\\) matrix.\nFinally, we can define the,\n\nDefinition 9 orthogonal column space (or orthogonal span) of \\(X\\) as, \\[\n\\mathcal{S}^{\\perp}(X) = \\{y\\in \\mathbb{R}^k:\\;y'x=0\\quad \\forall x\\in\\mathcal{S}(X)\\}\n\\]\n\n\n\n\nConsider a \\(n\\times k\\) matrix \\(X\\), the\n\nDefinition 10 row rank of \\(X\\) is the maximum number of linearly independent rows: \\[\nrowrank(X) \\leq n\n\\]\n\nWe say that matrix \\(X\\) has full row rank if \\(rowrank(X)=n\\).\nThe,\n\nDefinition 11 column rank of \\(X\\) is the maximum number of linearly independent columns:\n\\[\ncolrank(X) \\leq k\n\\]\n\nWe say that matrix \\(X\\) has full column rank if \\(colrank(X)=k\\).\nAn important result is,\n\nResult: the rank of \\(X\\): \\[\nr(X) = rowrank(X)=colrank(X) \\\\\n\\Rightarrow r(X)\\leq min\\{n,k\\}\n\\]\n\nIn addition, since the \\(r(X)\\) depends on the number of linearly independent columns, we can say that,\n\nResult: the dimension of \\(\\mathcal{S}(X)\\), \\(dim(\\mathcal{S}(X))\\), is given by the \\(r(X)\\).\n\nHere are a few additional results,\n\nResult: \\(r(X)=r(X')\\)\nResult: \\(r(XY)\\leq min\\{r(X),r(Y)\\}\\)\nResult: \\(r(XY)=r(X)\\) if \\(Y\\) is square and full rank\nResult: \\(r(X+Y)\\leq r(X) + r(Y)\\)\n\n\n\n\nConsider the case of a square, \\(n\\times n\\), matrix \\(A\\). We say that,\n\nDefinition 12 \\(A\\) is singular if the \\(r(A)&lt;n\\),\n\nor that,\n\nDefinition 13 \\(A\\) is non-singular if the \\(r(A)=n\\).\n\nThe singularity of a square matrix is important as it determines the invertibility of a matrix, which typically relates the existence of a unique solution in systems of linear equations. Here are a few key results,\n\nResult: There exists a matrix \\(B=A^{-1}\\), such that \\(AB=I_n\\) (where \\(I_n\\) is the identity matrix), if and only if \\(A\\) is non-singular.\nResult: \\(A\\) is non-singular if and only if the determinant of \\(A\\) is non-zero: \\(det(A)\\neq0\\).1\nResult: Likewise, \\(A\\) is singular if and only if \\(det(A)=0\\).\nResult: \\(AA^{-1}=A^{-1}A=I\\)\nResult: \\((A')^{-1}=(A^{-1})'\\)\nResult: If their respective inverses exist, then \\((AB)^{-1}=B^{-1}A^{-1}\\).\nResult: \\(det(AB)=det(A)det(B)\\)\nResult: \\(det(A^{-1})=det(A)^{-1}\\)\n\nFor any square matrix \\(A\\),\n\nDefinition 14 the trace of \\(A\\) is the sum of all diagonal elements: \\[\ntr(A) = \\sum_{i=1}^na_{ii}\n\\]\n\nRegarding the trace of a square matrix, here are a few important results:\n\nResult: \\(tr(A+B) = tr(A) + tr(B)\\)\nResult: \\(tr(\\lambda A) = \\lambda tr(A)\\) where \\(\\lambda\\) is a scalar\nResult: \\(tr(A) = tr(A')\\)\nResult: \\(tr(AB) = tr(BA)\\) where \\(AB\\) and \\(BA\\) are both square, but need not be of the same order.\nResult: \\(||A|| = (tr(A'A))^{1/2}\\)\n\n\n\n\nA symmetric matrix has the property that \\(A=A'\\). Therefore, \\(A\\) must be square.\nHere are a few important results concerning symmetric matrices.\n\nResult: \\(A^{-1}\\) exists if \\(det(A)\\neq 0\\) and \\(r(A)=n\\)\nResult: A is diagonalizable.2\nResult: The eigenvector decomposition of a square matrix gives you \\(A=C\\Lambda C^{-1}\\) where \\(\\Lambda\\) is a diagonal matrix of eigenvalues and $C$ a matrix of the corresponding eigenvectors. The symmetry of \\(A\\) gives you that \\(C^{-1}=C'\\Rightarrow A=C\\Lambda C'\\) with \\(C'C=CC'=I_{n}\\).3\n\nA key definition concerning symmetric matrices is their positive definiteness:\n\nDefinition 15 \\(A\\) is positive semi-definite if for any \\(x\\in\\mathbb{R}^n,\\;x'Ax\\geq0\\).\n\nGiven the eigenvector decomposition of a symmetric matrix, positive semi-definiteness implies \\(\\Lambda\\) is positive semi-definite: \\(\\lambda_i\\geq0\\quad\\forall i\\). Likewise,\n\nDefinition 16 \\(A\\) is positive definite if for any \\(x\\in\\mathbb{R}^n,\\;x'Ax&gt;0\\).\n\nAgain, based on the egeinvector decomposition, positive semi-definiteness implies \\(\\Lambda\\) is positive definite: \\(\\lambda_i&gt;0\\quad\\forall i\\).\nA few more results are:\n\nResult: \\(tr(A) = \\sum_{i=1}^n\\lambda_i\\)\nResult: \\(r(A) = r(\\Lambda)\\)\nResult: \\(det(A) = \\prod_{i=1}^n \\lambda_i\\)\n\nThis last result can be used to prove that any positive definite matrix is non-singular and therefore has an inverse.\nAny full-rank, positive semi-definite, symmetric matrix \\(B\\) has the additional properties:\n\nResult: \\(B=C\\Lambda C'\\) and \\(B^{-1} = C\\Lambda^{-1}C'\\)\nResult: We can define the square-root of \\(B\\) as \\(B^{1/2} = C\\Lambda^{1/2}C'\\). Similarly, \\(B^{-1/2} = C\\Lambda^{-1/2}C'\\).\n\n\n\n\nAn idempotent matrix has the property that \\(D=DD\\). Therefore, \\(D\\) must be square.\nHere are a few important results concerning idempotent matrices.\n\nResult: \\(D\\) is positive definite\nResult: \\(D\\) is diagonalizable\nResult: \\((I_n-D)\\) is also an idempotent matrix\nResult: With the exception of \\(I_n\\), all idempotent matrices are singular.\nResult: \\(r(D) = tr(D) = \\sum_{i=1}^n\\lambda_i\\)\nResult: \\(\\lambda_i\\in\\{0,1\\}\\quad \\forall\\;i\\)\n\nProjection matrices are idempotent, but need not be symmetric. However, for the purposes of this module we will deal exclusively with symmetric idempotent projection matrices.\n\n\n\nHere we will look at the derivatives of scalar with respect to (W.r.t.) a vector. You can also define other derivatives, such as the derivative of a vector w.r.t. a vector and the derivative of a scalar with respect to a matrix. However, these are not needed for these notes.\n\n\nSuppose \\(f(x)\\in R\\) (i.e. a scalar) and \\(x\\in R^n\\) (i.e. a \\(n\\times 1\\) vector). Then we can define the partial derivative of \\(f(x)\\) w.r.t. \\(x\\) as,\n\\[\n  \\frac{\\partial f(x)}{\\partial x}  = \\begin{bmatrix}\\frac{\\partial f(x)}{\\partial x_1} \\\\ \\frac{\\partial f(x)}{\\partial x_2} \\\\ \\vdots \\\\ \\frac{\\partial f(x)}{\\partial x_n} \\end{bmatrix}\n\\]\n\n\n\nA special case is when \\(f(x)\\) is linear in \\(x\\),\n\\[\nf(x) = a'x = \\sum_{i=1}^n a_ix_i\n\\] for \\(a\\in R^n\\). The derivative of \\(a'x\\) with respect to the vector \\(x\\) can be defined as,\n\\[\n  \\begin{aligned}\n  \\frac{\\partial a'x}{\\partial x} & = \\begin{bmatrix}\\frac{\\partial a'x}{\\partial x_1} \\\\ \\frac{\\partial a'x}{\\partial x_2} \\\\ \\vdots \\\\ \\frac{\\partial a'x}{\\partial x_n} \\end{bmatrix} \\\\\n  & = \\begin{bmatrix}a_1 \\\\ a_2 \\\\ \\vdots \\\\ a_n \\end{bmatrix} \\\\\n  & = a\n  \\end{aligned}\n\\] since the the partial derivate of \\(a'x = \\sum_{i=1}^n a_ix_i\\) w.r.t. \\(x_i\\) is just the scalar \\(a_i\\).\nThe derivative of a scalar w.r.t. to a vector yields a vector of partial derivatives.\nSince \\(a'x\\) is a scalar, it is by definition symmetric: \\(a'x = x'a\\). Thus,\n\\[\n\\frac{\\partial x'a}{\\partial x} = \\frac{\\partial a'x}{\\partial x} = a\n\\]\n\n\n\nSuppose \\(f(x)\\) is a linear transformation of \\(x\\),\n\\[\nf(x) = A'x\n\\] for any \\(m\\times n\\) matrix A,\n\\[\n  A = \\begin{bmatrix}a_1' \\\\ a_2' \\\\ \\vdots \\\\ a_m'\\end{bmatrix}\n\\] where \\(a_i\\in R^n\\;\\forall i=1,\\dots,m\\) and,\n\\[\n  Ax = \\begin{bmatrix}a_1'x \\\\ a_2'x \\\\ \\vdots \\\\ a_m'x\\end{bmatrix}\n\\] Note, \\(f(x)=Ax\\in R^m\\), a \\(m\\times 1\\) vector. We can then define,\n\\[\n  \\begin{aligned}\n  \\frac{\\partial Ax}{\\partial x'} & = \\begin{bmatrix}\\frac{\\partial a_1'x}{\\partial x_1} & \\frac{\\partial a_1'x}{\\partial x_2} & \\dots & \\frac{\\partial a_1'x}{\\partial x_n}\\\\ \\frac{\\partial a_2'x}{\\partial x_1} & \\frac{\\partial a_2'x}{\\partial x_2} & \\dots & \\frac{\\partial a_2'x}{\\partial x_n}\\\\ \\vdots & \\vdots & \\ddots & \\vdots\\\\ \\frac{\\partial a_m'x}{\\partial x_1} & \\frac{\\partial a_m'x}{\\partial x_2} & \\dots & \\frac{\\partial a_m'x}{\\partial x_n}\\\\ \\end{bmatrix} \\\\\n  & = \\begin{bmatrix} a_{11} & a_{12} & \\dots & a_{1n} \\\\ a_{21} & a_{22} & \\dots & a_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots\\\\ a_{m1} & a_{m2} & \\dots & a_{mn} \\\\ \\end{bmatrix} \\\\\n  & = A\n  \\end{aligned}\n\\]\nSince Ax is \\(m\\times 1\\) column vector, we take the derivative w.r.t. \\(x'\\) a row vector and not the column vector \\(x\\). This results in a matrix of partial derivatives.\n\n\n\nA second special case is where the function takes on the quadaratic form,\n\\[\nf(x) = x'Ax = \\sum_{i=1}^N\\sum_{j=1}^n a_{ij}x_ix_j\n\\]\nfor \\(n\\times n\\) (square) matrix A. As in the first linear case, \\(f(x)\\) is scalar.\nDefine \\(c = Ax\\), the \\(x'Ax = x'c\\). From the linear case, we know that,\n\\[\n\\frac{\\partial x'c}{\\partial x} = c\n\\]\nSimilarly, if we define \\(d = A'x\\) then \\(x'Ax = d'x\\). From the linear case, we know that,\n\\[\n\\frac{\\partial d'x}{\\partial x} = d\n\\]\nWe can define the total derivative as the sum of the partial derivatives w.r.t. to the first and second \\(x\\). Combining these two results, we have that,\n\\[\n\\frac{\\partial x'Ax}{\\partial x} = Ax + A'x\n\\]\nAnd if \\(A\\) is symmetric, this result simplifies to \\(2Ax\\)."
  },
  {
    "objectID": "material-linearalgebra.html#linear-dependence",
    "href": "material-linearalgebra.html#linear-dependence",
    "title": "Linear Algebra",
    "section": "",
    "text": "Consider a set of \\(k\\) \\(n\\)-dimensional vectors \\(\\{X_{1},X_{2},...,X_{k}\\}\\). These vectors are,\n\nDefinition 1 linearly dependent if there exists a set of scalars \\(\\{a_{1},a_{2},\\dots,a_{k}\\}\\) such that\n\\[\na_1X_1 + a_2X_2+\\ldots+a_kX_k=0\n\\]\nwhere at least one \\(a_i\\neq0\\).\n\nAlternatively, they are,\n\nDefinition 2 linearly independent if the only set of scalars \\(\\{a_{1},a_{2},\\dots,a_{k}\\}\\) that satisfies the above condition is \\(a_1,a_2,\\dots,a_k=0\\).\n\nIf we collect these \\(k\\) column-vectors in a matrix, \\(X=[X_1\\;X_2 \\dots X_k]\\), then the linear dependence condition can be written as,\n\\[\na_1X_1 + a_2X_2+\\ldots+a_kX_k=\\begin{bmatrix} X_1\\;X_2 \\dots X_k\\end{bmatrix}\\begin{bmatrix}a_1\\\\a_2\\\\\\vdots\\\\a_k\\end{bmatrix}=Xa = 0\n\\]\nGiven any \\(n\\times k\\) matrix \\(X\\), its columns are,\n\nDefinition 3 linearly dependent if there exists a vector \\(a\\in\\mathbb{R}^k\\) such that \\(a\\neq0\\) and \\(Xa=0\\);\n\nor,\n\nDefinition 4 linearly independent if the only vector \\(a\\in\\mathbb{R}^k\\) such that \\(Xa=0\\) is \\(a=0\\).\n\nFor any matrix there may be more than one vector \\(a\\in\\mathbb{R}^{k}\\) such that \\(Xa=0\\). Indeed, if both \\(a_{1},a_{2}\\in\\mathbb{R}^{k}\\)\nsatisfy this condition and \\(a_{1}\\neq a_{2}\\) then you can show that any linear combination of \\(\\{a_{1},a_{2}\\}\\) satisfies the\ncondition \\(X(a_{1}b_{1}+a_{2}b_{2})=0\\) for \\(b_{1},b_{2}\\in\\mathbb{R}\\). Thus, there exists an entire set of vectors which satisfy this condition. This set is referred to as the,\n\nDefinition 5 null space of \\(X\\), \\[\n\\mathcal{N}(X) = \\{a\\in\\mathbb{R}^k:\\;Xa=0\\}\n\\]\n\nIt should be evident from the definition that if the columns of \\(X\\) are linearly independent then \\(\\mathcal{N}(X)=\\{0\\}\\), a singleton. That is, it just includes the 0-vector."
  },
  {
    "objectID": "material-linearalgebra.html#vector-spaces-bases-and-spans",
    "href": "material-linearalgebra.html#vector-spaces-bases-and-spans",
    "title": "Linear Algebra",
    "section": "",
    "text": "Here, we concern ourselves only with real vectors from \\(\\mathbb{R}^n\\).\n\nDefinition 6 A vector space, denoted \\(\\mathcal{V}\\), refers to a set of vectors which is closed under finite addition and scalar multiplication.\n\n\nDefinition 7 A set of \\(k\\) linearly independent vectors, \\(\\{X_1,X_2,\\dots,X_k\\}\\), forms a basis for vector space \\(\\mathcal{V}\\) if \\(\\forall\\;y\\in\\mathcal{V}\\) there exists a set of \\(k\\) scalars such that, \\[\ny=X_1b_1+X_2b_2+\\ldots+X_kb_k\n\\]\n\nBased on these definitions, it is evident that for the Euclidean space, \\(\\mathbb{E}^n\\), any \\(n\\) linearly independent vectors from \\(\\mathbb{R}^n\\) is a basis. For example, any point in \\(\\mathbb{E}^2\\) can be defined as a multiple of,\n\\[\n\\begin{bmatrix}1\\\\0\\end{bmatrix}\\quad \\text{and} \\quad\\begin{bmatrix}0\\\\1\\end{bmatrix}\n\\]\nConsider again the \\(n\\times k\\) matrix \\(X\\), where \\(k&lt;n\\). Then we define the,\n\nDefinition 8 column space (or span) of \\(X\\), denoted \\(\\mathcal{S}(X)\\), as the vector space generate by the \\(k\\) columns of \\(X\\). Formally, \\[\n\\mathcal{S}(X) = \\{y\\in\\mathbb{R}^n:\\;y=Xb\\quad\\text{for some }b\\in \\mathbb{R}^k\\}\n\\]\n\nA property to note about the span or column space \\(X\\) is,\nResult: \\(\\mathcal{S}(X)=\\mathcal{S}(XX')\\) :::\nwhere \\(XX'\\) is a \\(n\\times n\\) matrix.\nFinally, we can define the,\n\nDefinition 9 orthogonal column space (or orthogonal span) of \\(X\\) as, \\[\n\\mathcal{S}^{\\perp}(X) = \\{y\\in \\mathbb{R}^k:\\;y'x=0\\quad \\forall x\\in\\mathcal{S}(X)\\}\n\\]"
  },
  {
    "objectID": "material-linearalgebra.html#rank",
    "href": "material-linearalgebra.html#rank",
    "title": "Linear Algebra",
    "section": "",
    "text": "Consider a \\(n\\times k\\) matrix \\(X\\), the\n\nDefinition 10 row rank of \\(X\\) is the maximum number of linearly independent rows: \\[\nrowrank(X) \\leq n\n\\]\n\nWe say that matrix \\(X\\) has full row rank if \\(rowrank(X)=n\\).\nThe,\n\nDefinition 11 column rank of \\(X\\) is the maximum number of linearly independent columns:\n\\[\ncolrank(X) \\leq k\n\\]\n\nWe say that matrix \\(X\\) has full column rank if \\(colrank(X)=k\\).\nAn important result is,\n\nResult: the rank of \\(X\\): \\[\nr(X) = rowrank(X)=colrank(X) \\\\\n\\Rightarrow r(X)\\leq min\\{n,k\\}\n\\]\n\nIn addition, since the \\(r(X)\\) depends on the number of linearly independent columns, we can say that,\n\nResult: the dimension of \\(\\mathcal{S}(X)\\), \\(dim(\\mathcal{S}(X))\\), is given by the \\(r(X)\\).\n\nHere are a few additional results,\n\nResult: \\(r(X)=r(X')\\)\nResult: \\(r(XY)\\leq min\\{r(X),r(Y)\\}\\)\nResult: \\(r(XY)=r(X)\\) if \\(Y\\) is square and full rank\nResult: \\(r(X+Y)\\leq r(X) + r(Y)\\)"
  },
  {
    "objectID": "material-linearalgebra.html#properties-of-square-matrices",
    "href": "material-linearalgebra.html#properties-of-square-matrices",
    "title": "Linear Algebra",
    "section": "",
    "text": "Consider the case of a square, \\(n\\times n\\), matrix \\(A\\). We say that,\n\nDefinition 12 \\(A\\) is singular if the \\(r(A)&lt;n\\),\n\nor that,\n\nDefinition 13 \\(A\\) is non-singular if the \\(r(A)=n\\).\n\nThe singularity of a square matrix is important as it determines the invertibility of a matrix, which typically relates the existence of a unique solution in systems of linear equations. Here are a few key results,\n\nResult: There exists a matrix \\(B=A^{-1}\\), such that \\(AB=I_n\\) (where \\(I_n\\) is the identity matrix), if and only if \\(A\\) is non-singular.\nResult: \\(A\\) is non-singular if and only if the determinant of \\(A\\) is non-zero: \\(det(A)\\neq0\\).1\nResult: Likewise, \\(A\\) is singular if and only if \\(det(A)=0\\).\nResult: \\(AA^{-1}=A^{-1}A=I\\)\nResult: \\((A')^{-1}=(A^{-1})'\\)\nResult: If their respective inverses exist, then \\((AB)^{-1}=B^{-1}A^{-1}\\).\nResult: \\(det(AB)=det(A)det(B)\\)\nResult: \\(det(A^{-1})=det(A)^{-1}\\)\n\nFor any square matrix \\(A\\),\n\nDefinition 14 the trace of \\(A\\) is the sum of all diagonal elements: \\[\ntr(A) = \\sum_{i=1}^na_{ii}\n\\]\n\nRegarding the trace of a square matrix, here are a few important results:\n\nResult: \\(tr(A+B) = tr(A) + tr(B)\\)\nResult: \\(tr(\\lambda A) = \\lambda tr(A)\\) where \\(\\lambda\\) is a scalar\nResult: \\(tr(A) = tr(A')\\)\nResult: \\(tr(AB) = tr(BA)\\) where \\(AB\\) and \\(BA\\) are both square, but need not be of the same order.\nResult: \\(||A|| = (tr(A'A))^{1/2}\\)"
  },
  {
    "objectID": "material-linearalgebra.html#properties-of-symmetric-matrices",
    "href": "material-linearalgebra.html#properties-of-symmetric-matrices",
    "title": "Linear Algebra",
    "section": "",
    "text": "A symmetric matrix has the property that \\(A=A'\\). Therefore, \\(A\\) must be square.\nHere are a few important results concerning symmetric matrices.\n\nResult: \\(A^{-1}\\) exists if \\(det(A)\\neq 0\\) and \\(r(A)=n\\)\nResult: A is diagonalizable.2\nResult: The eigenvector decomposition of a square matrix gives you \\(A=C\\Lambda C^{-1}\\) where \\(\\Lambda\\) is a diagonal matrix of eigenvalues and $C$ a matrix of the corresponding eigenvectors. The symmetry of \\(A\\) gives you that \\(C^{-1}=C'\\Rightarrow A=C\\Lambda C'\\) with \\(C'C=CC'=I_{n}\\).3\n\nA key definition concerning symmetric matrices is their positive definiteness:\n\nDefinition 15 \\(A\\) is positive semi-definite if for any \\(x\\in\\mathbb{R}^n,\\;x'Ax\\geq0\\).\n\nGiven the eigenvector decomposition of a symmetric matrix, positive semi-definiteness implies \\(\\Lambda\\) is positive semi-definite: \\(\\lambda_i\\geq0\\quad\\forall i\\). Likewise,\n\nDefinition 16 \\(A\\) is positive definite if for any \\(x\\in\\mathbb{R}^n,\\;x'Ax&gt;0\\).\n\nAgain, based on the egeinvector decomposition, positive semi-definiteness implies \\(\\Lambda\\) is positive definite: \\(\\lambda_i&gt;0\\quad\\forall i\\).\nA few more results are:\n\nResult: \\(tr(A) = \\sum_{i=1}^n\\lambda_i\\)\nResult: \\(r(A) = r(\\Lambda)\\)\nResult: \\(det(A) = \\prod_{i=1}^n \\lambda_i\\)\n\nThis last result can be used to prove that any positive definite matrix is non-singular and therefore has an inverse.\nAny full-rank, positive semi-definite, symmetric matrix \\(B\\) has the additional properties:\n\nResult: \\(B=C\\Lambda C'\\) and \\(B^{-1} = C\\Lambda^{-1}C'\\)\nResult: We can define the square-root of \\(B\\) as \\(B^{1/2} = C\\Lambda^{1/2}C'\\). Similarly, \\(B^{-1/2} = C\\Lambda^{-1/2}C'\\)."
  },
  {
    "objectID": "material-linearalgebra.html#properties-of-idempotent-matrices",
    "href": "material-linearalgebra.html#properties-of-idempotent-matrices",
    "title": "Linear Algebra",
    "section": "",
    "text": "An idempotent matrix has the property that \\(D=DD\\). Therefore, \\(D\\) must be square.\nHere are a few important results concerning idempotent matrices.\n\nResult: \\(D\\) is positive definite\nResult: \\(D\\) is diagonalizable\nResult: \\((I_n-D)\\) is also an idempotent matrix\nResult: With the exception of \\(I_n\\), all idempotent matrices are singular.\nResult: \\(r(D) = tr(D) = \\sum_{i=1}^n\\lambda_i\\)\nResult: \\(\\lambda_i\\in\\{0,1\\}\\quad \\forall\\;i\\)\n\nProjection matrices are idempotent, but need not be symmetric. However, for the purposes of this module we will deal exclusively with symmetric idempotent projection matrices."
  },
  {
    "objectID": "material-linearalgebra.html#vector-differentiation",
    "href": "material-linearalgebra.html#vector-differentiation",
    "title": "Linear Algebra",
    "section": "",
    "text": "Here we will look at the derivatives of scalar with respect to (W.r.t.) a vector. You can also define other derivatives, such as the derivative of a vector w.r.t. a vector and the derivative of a scalar with respect to a matrix. However, these are not needed for these notes.\n\n\nSuppose \\(f(x)\\in R\\) (i.e. a scalar) and \\(x\\in R^n\\) (i.e. a \\(n\\times 1\\) vector). Then we can define the partial derivative of \\(f(x)\\) w.r.t. \\(x\\) as,\n\\[\n  \\frac{\\partial f(x)}{\\partial x}  = \\begin{bmatrix}\\frac{\\partial f(x)}{\\partial x_1} \\\\ \\frac{\\partial f(x)}{\\partial x_2} \\\\ \\vdots \\\\ \\frac{\\partial f(x)}{\\partial x_n} \\end{bmatrix}\n\\]\n\n\n\nA special case is when \\(f(x)\\) is linear in \\(x\\),\n\\[\nf(x) = a'x = \\sum_{i=1}^n a_ix_i\n\\] for \\(a\\in R^n\\). The derivative of \\(a'x\\) with respect to the vector \\(x\\) can be defined as,\n\\[\n  \\begin{aligned}\n  \\frac{\\partial a'x}{\\partial x} & = \\begin{bmatrix}\\frac{\\partial a'x}{\\partial x_1} \\\\ \\frac{\\partial a'x}{\\partial x_2} \\\\ \\vdots \\\\ \\frac{\\partial a'x}{\\partial x_n} \\end{bmatrix} \\\\\n  & = \\begin{bmatrix}a_1 \\\\ a_2 \\\\ \\vdots \\\\ a_n \\end{bmatrix} \\\\\n  & = a\n  \\end{aligned}\n\\] since the the partial derivate of \\(a'x = \\sum_{i=1}^n a_ix_i\\) w.r.t. \\(x_i\\) is just the scalar \\(a_i\\).\nThe derivative of a scalar w.r.t. to a vector yields a vector of partial derivatives.\nSince \\(a'x\\) is a scalar, it is by definition symmetric: \\(a'x = x'a\\). Thus,\n\\[\n\\frac{\\partial x'a}{\\partial x} = \\frac{\\partial a'x}{\\partial x} = a\n\\]\n\n\n\nSuppose \\(f(x)\\) is a linear transformation of \\(x\\),\n\\[\nf(x) = A'x\n\\] for any \\(m\\times n\\) matrix A,\n\\[\n  A = \\begin{bmatrix}a_1' \\\\ a_2' \\\\ \\vdots \\\\ a_m'\\end{bmatrix}\n\\] where \\(a_i\\in R^n\\;\\forall i=1,\\dots,m\\) and,\n\\[\n  Ax = \\begin{bmatrix}a_1'x \\\\ a_2'x \\\\ \\vdots \\\\ a_m'x\\end{bmatrix}\n\\] Note, \\(f(x)=Ax\\in R^m\\), a \\(m\\times 1\\) vector. We can then define,\n\\[\n  \\begin{aligned}\n  \\frac{\\partial Ax}{\\partial x'} & = \\begin{bmatrix}\\frac{\\partial a_1'x}{\\partial x_1} & \\frac{\\partial a_1'x}{\\partial x_2} & \\dots & \\frac{\\partial a_1'x}{\\partial x_n}\\\\ \\frac{\\partial a_2'x}{\\partial x_1} & \\frac{\\partial a_2'x}{\\partial x_2} & \\dots & \\frac{\\partial a_2'x}{\\partial x_n}\\\\ \\vdots & \\vdots & \\ddots & \\vdots\\\\ \\frac{\\partial a_m'x}{\\partial x_1} & \\frac{\\partial a_m'x}{\\partial x_2} & \\dots & \\frac{\\partial a_m'x}{\\partial x_n}\\\\ \\end{bmatrix} \\\\\n  & = \\begin{bmatrix} a_{11} & a_{12} & \\dots & a_{1n} \\\\ a_{21} & a_{22} & \\dots & a_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots\\\\ a_{m1} & a_{m2} & \\dots & a_{mn} \\\\ \\end{bmatrix} \\\\\n  & = A\n  \\end{aligned}\n\\]\nSince Ax is \\(m\\times 1\\) column vector, we take the derivative w.r.t. \\(x'\\) a row vector and not the column vector \\(x\\). This results in a matrix of partial derivatives.\n\n\n\nA second special case is where the function takes on the quadaratic form,\n\\[\nf(x) = x'Ax = \\sum_{i=1}^N\\sum_{j=1}^n a_{ij}x_ix_j\n\\]\nfor \\(n\\times n\\) (square) matrix A. As in the first linear case, \\(f(x)\\) is scalar.\nDefine \\(c = Ax\\), the \\(x'Ax = x'c\\). From the linear case, we know that,\n\\[\n\\frac{\\partial x'c}{\\partial x} = c\n\\]\nSimilarly, if we define \\(d = A'x\\) then \\(x'Ax = d'x\\). From the linear case, we know that,\n\\[\n\\frac{\\partial d'x}{\\partial x} = d\n\\]\nWe can define the total derivative as the sum of the partial derivatives w.r.t. to the first and second \\(x\\). Combining these two results, we have that,\n\\[\n\\frac{\\partial x'Ax}{\\partial x} = Ax + A'x\n\\]\nAnd if \\(A\\) is symmetric, this result simplifies to \\(2Ax\\)."
  },
  {
    "objectID": "material-linearalgebra.html#footnotes",
    "href": "material-linearalgebra.html#footnotes",
    "title": "Linear Algebra",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThese notes do not cover how to calculate the determinant of a square matrix. You should be able to find a definition easily online.↩︎\nA matrix is diagonalizable if it is similar to some other diagonal matrix. Matrices \\(B\\) and \\(C\\) are similar if \\(C=PBP^{-1}\\). A square matrix which is not diagonalizable is defective. This property relates closely to eigenvector decomposition.↩︎\nRecall, an eigenvalue and eigenvector pair, \\((\\lambda,c)\\), of matrix \\(A\\) satisfy:\n\\[\nAc = \\lambda c\\Rightarrow (A-\\lambda I_n)c=0\n\\]↩︎"
  },
  {
    "objectID": "lecture-1-2.html",
    "href": "lecture-1-2.html",
    "title": "Understanding Causation",
    "section": "",
    "text": "Understanding Causation"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Econometrics 2: Microeconometrics",
    "section": "",
    "text": "This is not the official module website. The EC338 (2024/25) Moodle-page is the primary source of communication and resources for this module. All material available on this website can be found there.\nI designed this website as a means of testing out the Quarto package, developed by RStudio.1 This was partly as a way to develop my own skills and better incorporate the R language into the module. However, there are also accessibility benefits to this approach. Mathematical notation published in pdf’s using LaTeX cannot be read by a screen reader, while LaTeX published in html can. In addition, the website has built-in dark-mode option (see top-right toggle). For those who prefer pdf (for printing and notetaking), each page contains a unique downloadable link."
  },
  {
    "objectID": "index.html#welcome-to-338",
    "href": "index.html#welcome-to-338",
    "title": "Econometrics 2: Microeconometrics",
    "section": "",
    "text": "This is not the official module website. The EC338 (2024/25) Moodle-page is the primary source of communication and resources for this module. All material available on this website can be found there.\nI designed this website as a means of testing out the Quarto package, developed by RStudio.1 This was partly as a way to develop my own skills and better incorporate the R language into the module. However, there are also accessibility benefits to this approach. Mathematical notation published in pdf’s using LaTeX cannot be read by a screen reader, while LaTeX published in html can. In addition, the website has built-in dark-mode option (see top-right toggle). For those who prefer pdf (for printing and notetaking), each page contains a unique downloadable link."
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Econometrics 2: Microeconometrics",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nI would like to thank the following students for their help with this and other pedagogical projects: Diwen Si (BSc, 2023/24); Franklin Gonzalez Alcantara (BSc, 2023/24); Milan Makany (MSc, 2023/24); Jiarui Song (BSc, 2022/23); Patricio Hernandez Senosiain (BSc, 2022/23); Takumi Horikawa (MSc, 2022/23). You have been instrumental in my learning as well as the learning of the many students who will use these resources going forward.\nPlease let me know if you find any mistakes in my notes or code. In addition, I welcome suggestions on how to improve the R (or Stata) code provided in this module. I will endeavor to do my best to acknowledge the many people who have contributed to this material and my wider knowledge of this subject."
  },
  {
    "objectID": "index.html#source-material",
    "href": "index.html#source-material",
    "title": "Econometrics 2: Microeconometrics",
    "section": "Source Material",
    "text": "Source Material\nMy material for EC338 has been heavily influenced by Imbens and Rubin (2015) seminal book on causal inference, Causal Inference in Statistics, Social, and Biomedical Sciences. This book is dense and comprehensive, having been written for a wide range of scientists. It includes discussions on topics that are less relevant in economics research. I have borrowed extensively from Imbens and Rubin (2015) in my discussions of the Potential Outcomes Framework and randomized experiments, adopting much of their notation. The text is less relevant to natural experiment methodologies like difference-in-differences, but I have tried to maintain a consistent notation throughout.\nThere are a number of texts more readily used by applied economists. In particular, Angrist and Pischke (2009) Mostly Harmless Econometrics, a text that became a must-read for all applied microeconomics researchers during the last 15 years. In Mostly Harmless, Angrist and Pischke (2009) try to provide a practical guide to the use and interpretation of Econometric methods in applied microeconomics research. They focus on methods used by Economists in the study of randomized and natural experiments. This is both a strength and weakness of the book, as the content is largely based on practice (i.e. the methods being used by economists) which need not be “best practice”. As such, the book has the same shortcomings and biases as the economics discipline. For example, economists like linear regression models, but as we will soon see there are problems associated with the application of these models in a setting with heterogeneous treatment effects.\nIn my opinion, Angrist and Pischke (2009) is better suited to graduate students who have a more extensive knowledge of Econometric methods. Some of the Econometrics topics discussed as well as the notation used, is not typically covered in an undergraduate syllabus. Fortunately, Angrist has provided a less technical version, Mastering ’Metrics (Angrist 2014), focused on intuition and application.\nAnother modern text that includes helpful code and examples is Cunningham (2021) Causal Inference: The Mixtape. This book provides an excellent discussion of modern applied methods; especially topics related to the study of observational data (e.g., matching) and natural experiments (incl., DID, RDD, Event-studies). The discussion on difference-in-differences references some of the more recent literature related to heterogeneous effects.\nMy notes try to provide a more theoretical foundation to the discussion of causal inference in Econometrics, something that I feel is missing in Angrist and Pischke (2009) and Cunningham (2021). To do so, I borrow from Imbens and Rubin (2015), whose discussion of causal effects is based on an experimental model of the data generating process. This experimental foundation is not always consistent with the econometrics models studied in standard Econometric texts (see Wooldridge 2010). In my notes, I try to provide an introduction to the experimental model as well as a bridge to the econometrics models you are already familiar with (mostly linear regression model). But this bridge doesn’t always exist and, when it does, it sometimes takes a toll (i.e. additional assumptions).\nI hope you enjoy this module!\nNeil Lloyd"
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Econometrics 2: Microeconometrics",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.↩︎"
  },
  {
    "objectID": "lecture-1-1.html",
    "href": "lecture-1-1.html",
    "title": "Introduction",
    "section": "",
    "text": "Introduction"
  },
  {
    "objectID": "material-cef.html",
    "href": "material-cef.html",
    "title": "Conditional Expectation Function",
    "section": "",
    "text": "Consider the random variable \\(Y_i\\in\\mathbb{R}\\) and the random vector \\(X_i\\in\\mathbb{R}^k\\), \\(k\\geq1\\).1\n\n\nThe Conditional Expecation Function (CEF) - denoted \\(E[Y_i|X_i]\\) - is a random function. It is a function that returns the expected value of \\(Y_i\\) for each realized value of \\(X_i\\). Since \\(X_i\\) is a random vector the resulting function is random itself.\nIf we fix \\(X_i=x\\), then the value at which we are evaluating the function is no longer random. The result is a constant: the expected value of \\(Y_i\\) at the given \\(x\\).\n\\[\nE[Y_i|X_i=x] = \\int y \\cdot f_{Y}(y|X_i=x)dy = \\int y dF_{Y}(y|X_i=x)\n\\]\nThis follows the same logic that the expectation of a random variable is, \\(E[Y_i]\\), is not random.\nDiscrete case. The book devotes a lot time to the discussion of cases were \\(X_i\\) is a discrete random variable; using the notation \\(W_i\\in\\{0,1\\}\\) or \\(D_i\\in\\{0,1\\}\\). In this unique case, we can write the CEF as,\n\\[\nE[Y_i|D_i] = E[Y_i|D_i=0] + D_i\\cdot\\big(E[Y_i|D_i=1]-E[Y_i|D_i=0]\\big)\n\\]\nThe above function returns \\(E[Y_i|D_i=0]\\) when \\(D_i=0\\) and \\(E[Y_i|D_i=1]\\) when \\(D_i=1\\). This expression for the CEF will be useful in latter chapters of the book.\n\n\n\nThe Law of Iterated Expectations says that given two random variables2 \\([Y_i,X_i]\\), we can express the unconditional expected value of \\(Y_i\\) as the expected value of the conditional expectation of \\(Y_i\\) on \\(X_i\\).\n\\[\n        E[Y_i] = E\\big[E[Y_i|X_i]\\big]\n\\]\nWhere the outside expectation is with respect to \\(X_i\\),3 since the CEF is a random function of \\(X_i\\). We can expand this as follows,\n\\[\nE[Y_i] = \\int t \\cdot f_{Y_i}(t)dt = \\int\\int y \\cdot f_{Y_i|X}(y|x)dyf_X(x)dx = E\\big[E[Y_i|X_i]\\big]\n\\]\n\nExample 1 Suppose \\(Y_i\\) and \\(X_i\\) are both discrete, \\(Y_i\\in\\{1,2\\}\\) and \\(X_i\\in\\{3,4\\}\\), with the joint distribution:\n\n\\(f_{Y,X}\\)\n\n\n\n\\(X_i=3\\)\n\\(X_i=2\\)\n\n\n\\(Y_i=1\\)\n1/10\n3/10\n\n\n\\(Y_i=2\\)\n2/10\n4/10\n\n\n\nWe can then define the two marginal distributions,\n\n\\(f_Y\\)\n\n\n\\(Y_i=1\\)\n\\(Y_i=2\\)\n\n\n4/10\n6/10\n\n\n\nand,\n\n\\(f_X\\)\n\n\n\\(X_i=3\\)\n\\(X_i=4\\)\n\n\n3/10\n7/10\n\n\n\nLikewise, we know the conditional distribution \\(f_{Y|X}\\); which we get by dividing the joint distribution by the marginal distribution of \\(X_i\\). Each column of the conditional distribution should add up to 1.\n\n\\(f_{Y|X}\\)\n\n\n\n\\(X_i=3\\)\n\\(X_i=4\\)\n\n\n\\(Y_i=1\\)\n1/3\n3/7\n\n\n\\(Y_i=2\\)\n2/3\n4/7\n\n\n\nNow we can calculate the following objects:\n\n\\(E[Y_i]\\)\n\n\\[\n\\begin{aligned}\n        E[Y_i] =& 1\\cdot Pr(Y_i=1)+2\\cdot Pr(Y_i=2) \\\\\n        =&1\\cdot 4/10+2\\cdot 6/10 \\\\\n        =&16/10\n    \\end{aligned}\n\\]\n\n\\(E[Y_i|X_i=3]\\)\n\n\\[\n\\begin{aligned}\n        E[Y_i|X_i=3] =& 1\\cdot Pr(Y_i=1|X_i=3)+2\\cdot Pr(Y_i=2|X_i=3) \\\\\n        =&1\\cdot 1/3+2\\cdot 2/3 \\\\\n        =&5/3\n\\end{aligned}\n\\]\n\n\\(E[Y_i|X_i=4]\\)\n\n\\[\n\\begin{aligned}\n        E[Y_i|X_i=4] =& 1\\cdot Pr(Y_i=1|X_i=4)+2\\cdot Pr(Y_i=2|X_i=4) \\\\\n        =&1\\cdot 3/7+2\\cdot 4/7 \\\\\n        =&11/7\n    \\end{aligned}\n\\]\n\n\\(E\\big[E[Y_i|X_i]\\big]\\)\n\n\\[\n\\begin{aligned}\n        E\\big[E[Y_i|X_i]\\big] =& E[Y_i|X_i=3]\\cdot Pr(X_i=3)+ E[Y_i|X_i=4]\\cdot Pr(X_i=4) \\\\\n        =&5/3\\cdot3/10+11/7\\cdot 7/10 \\\\\n        =&16/10\n    \\end{aligned}\n\\]\nWe have therefore demonstrated the law of iterated expectations.\n\nWe can extend this principle to conditional expectations. Suppose you have three random variables/vectors \\(\\{Y_i,X_i,Z_i\\}\\), we can express the conditional expected value of \\(Y_i\\) on \\(X_i\\) as the (conditional) expected value of the conditional expectation of \\(Y_i\\) on \\(X_i\\) and \\(Z_i\\).\n\\[\n        E[Y_i|X_i] = E\\big[E[Y_i|X_i,Z_i]|X_i\\big]\n\\]\nHere the outside expectation is with respect \\(Z_i\\) conditional on \\(X_i\\). It utilizes the conditional distribution \\(f_{Z|X}\\) to form the outside expectation,\n\\[\nE[Y_i|X_i] = \\int y \\cdot f_{Y|X}(y|X_i)dt = \\int\\int y \\cdot f_{Y|X,Z}(y|X_i,z)dyf_{Z|X}(z|X_i)dz = E\\big[E[Y_i|X_i,Z_i]|X_i\\big]\n\\]\n\n\n\nThe following three theorems can be found in a range of Econometrics textbooks and Microeconometrics texts, including MM & MHE\n\nTheorem 1 We can express the observed outcome \\(Y_i\\) as a sum of \\(E[Y_i|X_i]+\\varepsilon_i\\) where \\(E[\\varepsilon_i|X_i]=0\\) (i.e., mean independent).\n\n\nProof. \n\n\\(E[\\varepsilon_i | X_i] = E[Y_i - E[Y_i | X_i] | X_i] = E[Y_i | X_i] - E[Y_i | X_i] = 0\\)\n\\(E[h(X_i)\\varepsilon_i] = E[h(X_i)E[\\varepsilon_i | X_i]] = E[h(X_i) \\times 0] = 0\\)\n\n\n\nTheorem 2 \\(E[Y_i|X_i]\\) is the best predictor of \\(Y_i\\).\n\n\nProof. \\[\n\\begin{aligned}\n(Y_i - m(X_i))^2 =& \\left((Y_i - E[Y_i | X_i]) + (E[Y_i | X_i] - m(X_i))\\right)^2 \\\\\n=& (Y_i - E[Y_i \\| X_i])^2 + (E[Y_i | X_i] - m(X_i))^2 \\\\&+ 2(Y_i - E[Y_i | X_i]) \\times (E[Y_i | X_i] - m(X_i))\n\\end{aligned}\n\\]\nThe last term (cross product) is mean zero. Thus, the function is minimized by setting \\(m(X_i) = E[Y_i | X_i]\\).\n\n\nTheorem 3 [ANOVA Theorem] The variance of \\(Y_i\\) can be decomposed as \\(V(E[Y_i|X_i])+E(V(Y_i|X_i))\\)\n\n\nProof. \\[\n\\begin{aligned}\n        V(Y_i)=&V(E[Y_i|X_i] + \\varepsilon_i) \\\\\n        =&V(E[Y_i|X_i])+V(\\varepsilon_i) \\\\\n        =&V(E[Y_i|X_i])+E[\\varepsilon_i^2]\n    \\end{aligned}\n\\] The second line follows from Theorem 1.1 (independence) and\n\\[\n        E[\\varepsilon_i^2]=E\\left[E[\\varepsilon_i^2|X_i]\\right]=E\\left[V(Y_i|X_i)\\right]\n\\]"
  },
  {
    "objectID": "material-cef.html#definition",
    "href": "material-cef.html#definition",
    "title": "Conditional Expectation Function",
    "section": "",
    "text": "The Conditional Expecation Function (CEF) - denoted \\(E[Y_i|X_i]\\) - is a random function. It is a function that returns the expected value of \\(Y_i\\) for each realized value of \\(X_i\\). Since \\(X_i\\) is a random vector the resulting function is random itself.\nIf we fix \\(X_i=x\\), then the value at which we are evaluating the function is no longer random. The result is a constant: the expected value of \\(Y_i\\) at the given \\(x\\).\n\\[\nE[Y_i|X_i=x] = \\int y \\cdot f_{Y}(y|X_i=x)dy = \\int y dF_{Y}(y|X_i=x)\n\\]\nThis follows the same logic that the expectation of a random variable is, \\(E[Y_i]\\), is not random.\nDiscrete case. The book devotes a lot time to the discussion of cases were \\(X_i\\) is a discrete random variable; using the notation \\(W_i\\in\\{0,1\\}\\) or \\(D_i\\in\\{0,1\\}\\). In this unique case, we can write the CEF as,\n\\[\nE[Y_i|D_i] = E[Y_i|D_i=0] + D_i\\cdot\\big(E[Y_i|D_i=1]-E[Y_i|D_i=0]\\big)\n\\]\nThe above function returns \\(E[Y_i|D_i=0]\\) when \\(D_i=0\\) and \\(E[Y_i|D_i=1]\\) when \\(D_i=1\\). This expression for the CEF will be useful in latter chapters of the book."
  },
  {
    "objectID": "material-cef.html#law-of-iterated-expectations",
    "href": "material-cef.html#law-of-iterated-expectations",
    "title": "Conditional Expectation Function",
    "section": "",
    "text": "The Law of Iterated Expectations says that given two random variables2 \\([Y_i,X_i]\\), we can express the unconditional expected value of \\(Y_i\\) as the expected value of the conditional expectation of \\(Y_i\\) on \\(X_i\\).\n\\[\n        E[Y_i] = E\\big[E[Y_i|X_i]\\big]\n\\]\nWhere the outside expectation is with respect to \\(X_i\\),3 since the CEF is a random function of \\(X_i\\). We can expand this as follows,\n\\[\nE[Y_i] = \\int t \\cdot f_{Y_i}(t)dt = \\int\\int y \\cdot f_{Y_i|X}(y|x)dyf_X(x)dx = E\\big[E[Y_i|X_i]\\big]\n\\]\n\nExample 1 Suppose \\(Y_i\\) and \\(X_i\\) are both discrete, \\(Y_i\\in\\{1,2\\}\\) and \\(X_i\\in\\{3,4\\}\\), with the joint distribution:\n\n\\(f_{Y,X}\\)\n\n\n\n\\(X_i=3\\)\n\\(X_i=2\\)\n\n\n\\(Y_i=1\\)\n1/10\n3/10\n\n\n\\(Y_i=2\\)\n2/10\n4/10\n\n\n\nWe can then define the two marginal distributions,\n\n\\(f_Y\\)\n\n\n\\(Y_i=1\\)\n\\(Y_i=2\\)\n\n\n4/10\n6/10\n\n\n\nand,\n\n\\(f_X\\)\n\n\n\\(X_i=3\\)\n\\(X_i=4\\)\n\n\n3/10\n7/10\n\n\n\nLikewise, we know the conditional distribution \\(f_{Y|X}\\); which we get by dividing the joint distribution by the marginal distribution of \\(X_i\\). Each column of the conditional distribution should add up to 1.\n\n\\(f_{Y|X}\\)\n\n\n\n\\(X_i=3\\)\n\\(X_i=4\\)\n\n\n\\(Y_i=1\\)\n1/3\n3/7\n\n\n\\(Y_i=2\\)\n2/3\n4/7\n\n\n\nNow we can calculate the following objects:\n\n\\(E[Y_i]\\)\n\n\\[\n\\begin{aligned}\n        E[Y_i] =& 1\\cdot Pr(Y_i=1)+2\\cdot Pr(Y_i=2) \\\\\n        =&1\\cdot 4/10+2\\cdot 6/10 \\\\\n        =&16/10\n    \\end{aligned}\n\\]\n\n\\(E[Y_i|X_i=3]\\)\n\n\\[\n\\begin{aligned}\n        E[Y_i|X_i=3] =& 1\\cdot Pr(Y_i=1|X_i=3)+2\\cdot Pr(Y_i=2|X_i=3) \\\\\n        =&1\\cdot 1/3+2\\cdot 2/3 \\\\\n        =&5/3\n\\end{aligned}\n\\]\n\n\\(E[Y_i|X_i=4]\\)\n\n\\[\n\\begin{aligned}\n        E[Y_i|X_i=4] =& 1\\cdot Pr(Y_i=1|X_i=4)+2\\cdot Pr(Y_i=2|X_i=4) \\\\\n        =&1\\cdot 3/7+2\\cdot 4/7 \\\\\n        =&11/7\n    \\end{aligned}\n\\]\n\n\\(E\\big[E[Y_i|X_i]\\big]\\)\n\n\\[\n\\begin{aligned}\n        E\\big[E[Y_i|X_i]\\big] =& E[Y_i|X_i=3]\\cdot Pr(X_i=3)+ E[Y_i|X_i=4]\\cdot Pr(X_i=4) \\\\\n        =&5/3\\cdot3/10+11/7\\cdot 7/10 \\\\\n        =&16/10\n    \\end{aligned}\n\\]\nWe have therefore demonstrated the law of iterated expectations.\n\nWe can extend this principle to conditional expectations. Suppose you have three random variables/vectors \\(\\{Y_i,X_i,Z_i\\}\\), we can express the conditional expected value of \\(Y_i\\) on \\(X_i\\) as the (conditional) expected value of the conditional expectation of \\(Y_i\\) on \\(X_i\\) and \\(Z_i\\).\n\\[\n        E[Y_i|X_i] = E\\big[E[Y_i|X_i,Z_i]|X_i\\big]\n\\]\nHere the outside expectation is with respect \\(Z_i\\) conditional on \\(X_i\\). It utilizes the conditional distribution \\(f_{Z|X}\\) to form the outside expectation,\n\\[\nE[Y_i|X_i] = \\int y \\cdot f_{Y|X}(y|X_i)dt = \\int\\int y \\cdot f_{Y|X,Z}(y|X_i,z)dyf_{Z|X}(z|X_i)dz = E\\big[E[Y_i|X_i,Z_i]|X_i\\big]\n\\]"
  },
  {
    "objectID": "material-cef.html#properties-of-the-cef",
    "href": "material-cef.html#properties-of-the-cef",
    "title": "Conditional Expectation Function",
    "section": "",
    "text": "The following three theorems can be found in a range of Econometrics textbooks and Microeconometrics texts, including MM & MHE\n\nTheorem 1 We can express the observed outcome \\(Y_i\\) as a sum of \\(E[Y_i|X_i]+\\varepsilon_i\\) where \\(E[\\varepsilon_i|X_i]=0\\) (i.e., mean independent).\n\n\nProof. \n\n\\(E[\\varepsilon_i | X_i] = E[Y_i - E[Y_i | X_i] | X_i] = E[Y_i | X_i] - E[Y_i | X_i] = 0\\)\n\\(E[h(X_i)\\varepsilon_i] = E[h(X_i)E[\\varepsilon_i | X_i]] = E[h(X_i) \\times 0] = 0\\)\n\n\n\nTheorem 2 \\(E[Y_i|X_i]\\) is the best predictor of \\(Y_i\\).\n\n\nProof. \\[\n\\begin{aligned}\n(Y_i - m(X_i))^2 =& \\left((Y_i - E[Y_i | X_i]) + (E[Y_i | X_i] - m(X_i))\\right)^2 \\\\\n=& (Y_i - E[Y_i \\| X_i])^2 + (E[Y_i | X_i] - m(X_i))^2 \\\\&+ 2(Y_i - E[Y_i | X_i]) \\times (E[Y_i | X_i] - m(X_i))\n\\end{aligned}\n\\]\nThe last term (cross product) is mean zero. Thus, the function is minimized by setting \\(m(X_i) = E[Y_i | X_i]\\).\n\n\nTheorem 3 [ANOVA Theorem] The variance of \\(Y_i\\) can be decomposed as \\(V(E[Y_i|X_i])+E(V(Y_i|X_i))\\)\n\n\nProof. \\[\n\\begin{aligned}\n        V(Y_i)=&V(E[Y_i|X_i] + \\varepsilon_i) \\\\\n        =&V(E[Y_i|X_i])+V(\\varepsilon_i) \\\\\n        =&V(E[Y_i|X_i])+E[\\varepsilon_i^2]\n    \\end{aligned}\n\\] The second line follows from Theorem 1.1 (independence) and\n\\[\n        E[\\varepsilon_i^2]=E\\left[E[\\varepsilon_i^2|X_i]\\right]=E\\left[V(Y_i|X_i)\\right]\n\\]"
  },
  {
    "objectID": "material-cef.html#footnotes",
    "href": "material-cef.html#footnotes",
    "title": "Conditional Expectation Function",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe subscript \\(i\\) is not necessary here. However, this notation is consistent with the rest of the book. In this book, \\(Y_i\\) denotes a random variable, \\(\\in \\mathbb{R}\\), and \\(Y\\) a random vector, \\(\\in \\mathbb{R}^n\\). Likewise, \\(X_i\\) is a random vector, \\(\\in \\mathbb{R}^k\\), while \\(X\\) will represent a random matrix, \\(\\in \\mathbb{R}^n \\times \\mathbb{R}^k\\).↩︎\nThis can be extended to random vectors.↩︎\nSome texts use the notation \\(E_X\\big[E[Y_i|X_i]\\big]\\) to demonstrate that the outside expectation is with respect to \\(X_i\\).↩︎"
  },
  {
    "objectID": "seminar-1.html",
    "href": "seminar-1.html",
    "title": "Seminar 1: Power Calculations and Simulations",
    "section": "",
    "text": "library(tidyverse)\nlibrary(modelsummary)\nlibrary(tinytable)\nlibrary(psych)"
  },
  {
    "objectID": "seminar-1.html#required-r-packages",
    "href": "seminar-1.html#required-r-packages",
    "title": "Seminar 1: Power Calculations and Simulations",
    "section": "",
    "text": "library(tidyverse)\nlibrary(modelsummary)\nlibrary(tinytable)\nlibrary(psych)"
  },
  {
    "objectID": "seminar-1.html#learning-objectives",
    "href": "seminar-1.html#learning-objectives",
    "title": "Seminar 1: Power Calculations and Simulations",
    "section": "2 Learning Objectives",
    "text": "2 Learning Objectives\nIn this seminar you will learn how to:\n\nmake an a priori power calculation for an experiment;\nsimulate an experiment in R/Stata;\nand visualize the properties of the power function."
  },
  {
    "objectID": "seminar-1.html#motivation",
    "href": "seminar-1.html#motivation",
    "title": "Seminar 1: Power Calculations and Simulations",
    "section": "3 Motivation",
    "text": "3 Motivation\nStatistical power is an valuable theoretical and practical concept. The power of a test is the probability of rejecting the null hypothesis when the alternative hypothesis is true.\nLet’s consider the simple null hypothesis of no average treatment effect,1\n\\[\n  H_0: \\tau_{ATE} = 0\n\\]\nWhen testing this null hypothesis against the alternative,\n\\[\nH_1: \\tau_{ATE} \\neq 0\n\\]\nthe set of values that fulfill the alternative hypothesis is any value on the real line, excluding 0.\nThe power of this test is,\n\\[\n  Pr\\big(\\text{Reject }H_0|H_1\\text{ is true}\\big) = Pr\\big(\\text{Reject }H_0|\\tau_{ATE} = \\tau_0; \\tau_0\\neq 0\\big)\n\\]\nWhen do we reject \\(H_0\\)? A valid test is defined by a rejection rule based on a test static and a corresponding critical value. For example, in this two-sided test,\n\\[\n\\text{Reject }H_0\\text{ if}\\qquad \\bigg| \\frac{\\hat{\\tau}}{se(\\hat{\\tau})}\\bigg|&gt;z_{1-\\alpha/2}\n\\]\nIn statistical inference there is a trade off between Type I and II errors. We limit Type I erros - probability of rejecting \\(H_0\\) when \\(H_0\\) is true - through the choice of \\(\\alpha\\): the significance level. In Economics, this is typically chosen to be 1%, 5%, or 10%.\n\\[\n\\alpha = Pr\\big(\\text{Reject }H_0|H_0\\text{ is true}\\big)\n\\] A test is said to be of size \\(\\alpha\\) if the probability of a Type I error is \\(\\leq \\alpha\\).\nThe finite sample distribution of this test statistic is not known as we do not know the distributions of \\(\\{Y(1),Y(0)\\}\\) upon which the test statistic is based.2 By the Central Limit Theorem, we know that the limiting distribution is standard normal under the null hypothesis. This is true only under \\(H_0\\). The CLT holds for the true mean of estimator (\\(E[\\hat{\\tau}]=\\tau_{ATE}\\)), which we know under \\(H_0\\) to be \\(0\\).\nThus, under \\(H_0\\),\n\\[\n\\frac{\\hat{\\tau}}{se(\\hat{\\tau})} = \\frac{\\hat{\\tau}-E[\\hat{\\tau}]}{se(\\hat{\\tau})} \\overset{d}\\longrightarrow N(0,1) \\quad \\text{as }n\\rightarrow \\infty\n\\] For this reason, we can approximate the distribution of the test statistic with the standard normal.\n\\[\n\\frac{\\hat{\\tau}}{se(\\hat{\\tau})} \\quad a.\\thicksim N(0,1) \\quad \\text{under }H_0\n\\]\n\nExercise 1 Can you show that the size of this test is \\(\\alpha\\) (as \\(n\\rightarrow \\infty\\))?\n\nUnfortunatley, we cannot restrict the probability of both a Type I and II error. Here, we chose the critical values to give us a test of size \\(\\alpha\\). By doing so, we give up the possibility of limiting the probability of a Type II error. Moreover, a smaller \\(\\alpha\\) will necessarily imply a higher probability of Type II error.3\nThe probability of a Type II error depends on the power of the test.\n\\[\nPr\\big(\\text{Failing to reject }H_0|H_0\\text{ is false}\\big) = 1 - Pr\\big(\\text{Reject }H_0|H_1\\text{ is true}\\big)\n\\]\nA more powerful test has a lower probability of Type II error. Since, two tests might have the same size (i.e., \\(\\alpha\\)) we strictly prefer the more powerful test. Power is therefore an important metric by which we can compare different tests.\nFor this test (\\(H_0: \\tau_{ATE}=0\\)), the power of the test tells you how likely you are to detect a given non-zero average treatment effect in the experiment. Given how expensive it is to implement experiments (especially in Economics), this is useful information. There would be no point running a RCT if the likelihood of rejecting a null effect is very low."
  },
  {
    "objectID": "seminar-1.html#power-calculation",
    "href": "seminar-1.html#power-calculation",
    "title": "Seminar 1: Power Calculations and Simulations",
    "section": "4 Power calculation",
    "text": "4 Power calculation\nIn this setting, the power function tells you the probability of rejecting \\(H_0\\) for any true value of the ATE; denoted here by the scalar \\(\\tau_0\\). For the above two-sided test, this is given by,\n\\[\n\\begin{aligned}\n        \\theta(\\tau_0)=&Pr(\\text{Reject }H_0|\\tau_{ATE}=\\tau_0) \\\\\n        =& Pr\\left(\\bigg|\\frac{\\hat{\\tau}}{se(\\hat{\\tau})}\\bigg|&gt;z_{1-\\alpha/2}\\bigg|\\tau_{ATE}=\\tau_0\\right) \\\\\n        =&Pr\\left(\\frac{\\hat{\\tau}}{se(\\hat{\\tau})}&lt;z_{\\alpha/2}\\bigg|\\tau_{ATE}=\\tau_0\\right)+Pr\\left(\\frac{\\hat{\\tau}}{se(\\hat{\\tau})}&gt;z_{1-\\alpha/2}\\bigg|\\tau_{ATE}=\\tau_0\\right) \\\\\n        =&Pr\\left(\\frac{\\hat{\\tau}-\\tau_0}{se(\\hat{\\tau})}+\\frac{\\tau_0}{se(\\hat{\\tau})}&lt;z_{\\alpha/2}\\bigg|\\tau_{ATE}=\\tau_0\\right)+Pr\\left(\\frac{\\hat{\\tau}-\\tau_0}{se(\\hat{\\tau})}+\\frac{\\tau_0}{se(\\hat{\\tau})}&gt;z_{1-\\alpha/2}\\bigg|\\tau_{ATE}=\\tau_0\\right) \\\\\n        \\approx& Pr\\left(Z&lt;z_{\\alpha/2}-\\frac{\\tau_0}{se(\\hat{\\tau})}\\bigg|\\tau_{ATE}=\\tau_0\\right)+Pr\\left(Z&gt;z_{1-\\alpha/2}-\\frac{\\tau_0}{se(\\hat{\\tau})}\\bigg|\\tau_{ATE}=\\tau_0\\right)\n    \\end{aligned}\n\\]\nwhere \\(Z\\) is a standard normal random variable. The final equality is an approximation since we do not know the finite sample distribution of the test statistic.\n\nExercise 2 Given the above definition of the power function, what is \\(\\theta(0)\\)?\n\nTo make further use of this function, we need to define \\(\\hat{\\tau}\\) and \\(se(\\hat{\\tau})\\). Consider,\n\\[\n\\begin{aligned}\n  \\hat{\\tau} =& \\frac{1}{N_t} \\sum_{i:W_i=1}Y_i-\\frac{1}{N_c} \\sum_{i:W_i=0}Y_i \\\\\n  se(\\hat{\\tau}) =& \\sqrt{V^{cons}} \\\\\n  =&\\sqrt{\\sigma^2\\left(\\frac{1}{N_t}+\\frac{1}{N_c}\\right)}\n\\end{aligned}\n\\]\nHere we are using the constant-variance finite sample estimator. One cannot use the Neyman estimator without prior knowledge the variance of \\(Y(1)\\), the potential outcome with treatment. For this reason, it is less useful in power calculations made prior to the experiment.4\n\nExercise 3 A researcher has a sample of 650 individuals: \\(N_t = 270\\) and \\(N_c = 380\\). Using the above formulae, calculate the power of the experiment to detect a treatment effect equal to 15% of a standard deviation of the outcome variable.\n\n\nExercise 4 A researcher needs to show that their test has 80% power for a treatment effect of 20% of a standard deviation of the outcome. Using the above formulae, compute the sample size needed to achieve this level of power when assignment into treatment is equal. Then compute the required sample size if only a third of the sample is treated.\n\n\nExercise 5 How could you improve the power of an experiment?"
  },
  {
    "objectID": "seminar-1.html#simulation-of-power-function",
    "href": "seminar-1.html#simulation-of-power-function",
    "title": "Seminar 1: Power Calculations and Simulations",
    "section": "5 Simulation of power function",
    "text": "5 Simulation of power function\nIn this section we will simulate an experiment and plot the power function of the OLS estimator from a simple linear regression model.5 We will also examine the impact of adding good controls (covariates) to the estimated model.\n\n5.1 Setup\nWe begin by constructing a vector of potential outcomes, \\(\\{Y(0),Y(1)\\}\\), based on a known data generating process (DGP) and treatment effect. We will assign values to \\(Y(0)\\) based on the following linear DGP,\n\\[\n        Y_i(0) = \\alpha + X_i'\\gamma + \\varepsilon_i\n\\]\nwhere,\n\n\\(Y_i(0)\\) is the potential outcome without treatment and is measured after treatment has taken place. In this exercise we assume that the outcome is the log of wages.\n\\(X_i\\) is a vector of covariates that explain some of the variation in \\(Y_i(0)\\). In this instance, we will treat it as a linear term in age and gender dummy.\nThe error term will be drawn from a distribution of our choosing.\n\nFor this exercise, the relevant parameters \\(\\{\\alpha,\\gamma',\\sigma_{\\varepsilon}\\}\\) have been chosen to match a linear model of log wages against a dummy variable for gender and linear term in age, using data from the 2017 US CPS.\nWe can then generate the observed outcome as,\n\\[\n    Y_i = Y_i(0) + \\tau W_i\n\\]\nwhere \\(W_i\\) is an indicator function denoting treatment status in a completely randomized experiment with equal allocation to treatment and control. We will specify the homogeneous treatment to be \\(\\tau = 2\\%\\).6\n\n\n5.2 Data generating process\nStart by setting a seed. This ensures that all random number generators in this programme are replicable.\n\nset.seed(12956)\n\nGenerate an empty dataframe with 200 observations.\n\ndata1 &lt;- data.frame(matrix(ncol = 0, nrow = 200))\n\nDefine the parameters of the simulation.\n\nalpha &lt;- 0.07\ngamma_f &lt;- -0.17\ngamma_a &lt;- 0.08\ntau &lt;- 0.02\n\nAfter having selected each parameter we need to draw values of each explanatory variable, including the error term, in order to calculate the value of the potential outcomes. In this instance we will make use of known distributions.\nThe error term will be drawn from a normal distribution \\(N(0,0.55)\\). Gender will be drawn from a binonmial distribution while age will be a sequence of integers assigned based on a uniform distribution. As each variable is drawn independently, the covariance of the two variables should be zero. However, the covariance in the realized sample will be non-zero.\n\ndata1$female &lt;- rbinom(200,1,0.5)\ndata1$age &lt;- floor(46*runif(200) + 20)\ndata1$error &lt;- rnorm(200,0,0.55)\n\nGenerate Y(0)\n\ndata1$Y0 &lt;- alpha + gamma_f*data1$female + gamma_a*data1$age + data1$error\n\nTo control the number of observations assigned to treatment and control we will not use the binomial distribution command. Instead we will use a uniformly distributed number to sort and then group the data. Note, there are many way to do this assignment.\nWe can then apply the treatment and generate the observable outcome variable.\n\ndata1$temp &lt;- runif(200)\ndata1$treat &lt;- ifelse(data1$temp&gt;=median(data1$temp),1,0)\ntable(data1$treat)\n\n\n  0   1 \n100 100 \n\n\nGenerate the observable outcome variable\n\ndata1$Yobs &lt;- data1$Y0 + tau*data1$treat\n\nWe can now estimate the treatment effect using a linear regression model, with and without the additional covariates.\n\nreg1 &lt;- lm(Yobs ~ treat, data=data1)\nreg2 &lt;- lm(Yobs ~ treat + female + age, data=data1)\nmodelsummary(list(\"(1)\"=reg1,\"(2)\"=reg2), stars=c('*'=.1, '**'=.05,'***'=.01), gof_map = c(\"nobs\", \"r.squared\", \"rmse\"))\n\n \n\n  \n    \n    \n    tinytable_6pi48zj88msnj88p7u2w\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n        \n              \n                 \n                (1)\n                (2)\n              \n        \n        * p &lt; 0.1, ** p &lt; 0.05, *** p &lt; 0.01\n        \n                \n                  (Intercept)\n                  3.397***\n                  0.094   \n                \n                \n                             \n                  (0.120) \n                  (0.146) \n                \n                \n                  treat      \n                  -0.018  \n                  -0.003  \n                \n                \n                             \n                  (0.170) \n                  (0.082) \n                \n                \n                  female     \n                          \n                  -0.182**\n                \n                \n                             \n                          \n                  (0.083) \n                \n                \n                  age        \n                          \n                  0.079***\n                \n                \n                             \n                          \n                  (0.003) \n                \n                \n                  Num.Obs.   \n                  200     \n                  200     \n                \n                \n                  R2         \n                  0.000   \n                  0.770   \n                \n                \n                  RMSE       \n                  1.20    \n                  0.57    \n                \n        \n      \n    \n\n    \n\n  \n\n\n\n\nWhy are the two estimates of \\(\\tau\\) - \\(\\hat{\\beta}_1^{OLS}\\) and \\(\\hat{\\beta}_2^{OLS}\\) - not equal to 0.02? The OLS estimator is a random variable that has a distribution. The unconfoundedness of the treatment assignment and the homogeneous treatment effects mean that in expectation these estimators are unbiased and equal to 0.02. However, a particular realization may not be. In fact, the probability that \\(\\hat{\\beta}^{OLS}=\\tau_{ATE}=0.2\\) is 0.\n\n\n5.3 Monte Carlo simulation\nTo examine this characteristic of both estimators we will repeat the above process R-times. This will allow us to observe the overall distribution of the estimator. To do so we will write a short program that executes the same process R-times. The programme stores value of the estimator in a matrix.\n\nmat1 &lt;- replicate(1000, {\n  df&lt;-data.frame(matrix(ncol = 0, nrow = 200));\n  df$female &lt;- rbinom(200,1,0.5);\n  df$age &lt;- floor(46*runif(200) + 20);\n  df$error &lt;- rnorm(200,0,0.55);\n  df$Y0 &lt;- alpha + gamma_f*df$female + gamma_a*df$age + df$error;\n  df$temp &lt;- runif(200);\n  df$treat &lt;- ifelse(df$temp&gt;=median(df$temp),1,0);\n  df$Yobs &lt;- df$Y0 + tau*df$treat;\n  lm1 &lt;- lm(Yobs ~ treat, data=df);\n  lm2 &lt;- lm(Yobs ~ treat + female + age, data=df);\n  coef &lt;- c(lm1$coefficients[2],lm2$coefficients[2])\n}, simplify = \"array\")\nmat2 &lt;-t(mat1)\ncolnames(mat2)&lt;-c(\"beta1\",\"beta2\")\nsummary(mat2)\n\n     beta1              beta2         \n Min.   :-0.50429   Min.   :-0.22257  \n 1st Qu.:-0.09083   1st Qu.:-0.03110  \n Median : 0.02168   Median : 0.01811  \n Mean   : 0.01898   Mean   : 0.02053  \n 3rd Qu.: 0.12914   3rd Qu.: 0.07469  \n Max.   : 0.52228   Max.   : 0.28544  \n\ndescribe(mat2)\n\n      vars    n mean   sd median trimmed  mad   min  max range  skew kurtosis\nbeta1    1 1000 0.02 0.17   0.02    0.02 0.16 -0.50 0.52  1.03 -0.07     0.06\nbeta2    2 1000 0.02 0.08   0.02    0.02 0.08 -0.22 0.29  0.51 -0.02    -0.01\n        se\nbeta1 0.01\nbeta2 0.00\n\n\nHaving repeated the process 1000 times we can now examine the characteristics of each estimator, first by summarizing the stored values and then graphically. First, using a histogram:\n\nggplot(as.data.frame(mat2), aes(x=beta1)) +\n  geom_histogram(colour=\"black\", fill=\"lightgrey\") +\n  geom_vline(aes(xintercept=mean(beta1)),\n            color=\"red\", linetype=\"dashed\", size=1)\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nggplot(as.data.frame(mat2), aes(x=beta2)) +\n  geom_histogram(colour=\"black\", fill=\"lightgrey\") +\n  geom_vline(aes(xintercept=mean(beta2)),\n            color=\"red\", linetype=\"dashed\", size=1)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nSecond, using a kernel density function, which generates a smoothed estimate of the density function.\n\nggplot(as.data.frame(mat2), aes(x=beta1)) +\n  geom_density(aes(x=beta1,color=\"Beta1\")) +\n  geom_density(aes(x=beta2,color=\"Beta2\"),linetype=\"dashed\") +\n  geom_vline(aes(xintercept=0.02,color=\"tau=0.02\")) +\n  scale_color_manual(\"\",\n                     breaks=c(\"Beta1\",\"Beta2\",\"tau=0.02\"),\n                     values=c(\"Beta1\"=\"red\", \"Beta2\"=\"blue\",\"tau=0.02\"=\"darkgreen\")) +\n  xlab(\"beta/tau\") +\n  labs(title=\"kernel density plot\")\n\n\n\n\n\n\n\n\nThe distribution of both estimators is remarkably . This is by design. The distribution of \\(\\hat{\\beta}\\) is dependent on the distribution of the error term. In this instance, that is a normal distribution because of the DGP we specified.\n\n\n5.4 Power function\nTo compute the power function we need to know the variance of each of our two OLS estimators. Given that the variance of the estimator is in fact known, we could include this directly. Alternatively, we could estimate the variance of the each estimator using the distribution from the simulation.\nLet us see how close the simulation variance is to the true DGP variance. From the DGP, we know that\n\\[\n\\begin{aligned}\n    Var(\\hat{\\beta}^{OLS}_1) &= \\frac{\\gamma_f^2Var(female_i)+\\gamma_a^2Var(age_i)+\\sigma_{\\varepsilon}^2}{N(\\bar{W}-\\bar{W}^2)} \\\\\n    Var(\\hat{\\beta}^{OLS}_2) &= \\frac{\\sigma_{\\varepsilon}^2}{N(\\bar{W}-\\bar{W}^2)}\n\\end{aligned}\n\\] where, \\[\n    \\begin{aligned}\n    Var(female_i) &= 0.5(1-0.5) \\\\\n    Var(age_i) &= \\frac{1}{12}(65-20)^2\n\\end{aligned}\n\\]\nCalculate standard error for each estimator based on known distributions.\n\nse1 &lt;- sqrt((gamma_f^2*(0.5-0.5^2) + gamma_a^2*1/12*(65-20)^2 + 0.55^2)/(200*(0.5-0.5^2)))\nse2 &lt;- sqrt((0.55^2)/(200*(0.5-0.5^2)))\nse1\n\n[1] 0.1667168\n\nse2\n\n[1] 0.07778175\n\n\nApproximate the standard error using the standard deviation of the simulations.\n\nsd1 &lt;- sd(as.data.frame(mat2)$beta1)\nsd1\n\n[1] 0.168046\n\nsd2 &lt;- sd(as.data.frame(mat2)$beta2)\nsd2\n\n[1] 0.07764108\n\n\nFor now we will proceed to calculate the power function using the known DGP variance. To do so we will first construct a grid of alternative values of \\(beta\\). Next, we will estimate the power for each true value of \\(beta\\); i.e. each value in the grid. Finally, we will plot the two power functions alongside one another.\nConstruct grid of alternative values of \\(\\tau_0\\), centered around \\(\\tau_0\\)=0.02. This should include \\(\\tau_0=0\\), the null hypothesis.\n\ndata1$tau0[1]&lt;- -0.1 + tau\nfor (i in 2:200) {\n  data1$tau0[i] &lt;- data1$tau0[i-1] + 0.001\n}\n\nCalculate power at each value of \\(\\tau_0\\).\n\ndata1$theta1 &lt;- pnorm(qnorm(0.025)-data1$tau0/se1) + 1-pnorm(qnorm(0.975)-data1$tau0/se1)\ndata1$theta2 &lt;- pnorm(qnorm(0.025)-data1$tau0/se2) + 1-pnorm(qnorm(0.975)-data1$tau0/se2)\n\nPlot power curves\n\nggplot(data1, aes(x=tau0,y=theta1)) +\n  geom_line(aes(y=theta1,color=\"Beta1\")) +\n  geom_line(aes(y=theta2,color=\"Beta2\"),linetype=\"dashed\") +\n  geom_vline(aes(xintercept=0,color=\"H0\")) +\n  geom_vline(aes(xintercept=0.02,color=\"tau=0.02\")) +\n  geom_hline(aes(yintercept=0.05,color=\"a=0.05\")) +\n  scale_color_manual(\"\",\n                     breaks=c(\"Beta1\",\"Beta2\",\"H0\",\"tau=0.02\",\"a=0.05\"),\n                     values=c(\"Beta1\"=\"red\", \"Beta2\"=\"blue\",\"H0\"=\"black\",\n                              \"tau=0.02\"=\"darkgreen\",\"a=0.05\"=\"darkgrey\")) +\n  xlab(\"beta/tau\") +\n  ylab(\"power\") +\n  labs(title=\"power curves\")\n\n\n\n\n\n\n\n\nIt is clear from the displayed power functions that the second estimator is universally more powerful. The lower variance of this estimators translates directly into a higher probability of rejecting the null hypothesis of no-effect for all non-zero values of the true hypothesis. It is for this reason that we strongly prefer the second estimator and do not simply estimate a model of outcome on treatment.\nIn this instance, there is relatively little power to reject the null at the true value. Can you calculate the power at \\(\\tau_0=0.02\\)?\n\n\n5.5 Sample size\nWe have explored the value of including covariates, but what about increasing the sample size? Repeat the above process with a sample size of \\(N=1000\\) and compare the respective distributions and power functions of the estimators.\n\nmat3 &lt;- replicate(1000, {\n  df2&lt;-data.frame(matrix(ncol = 0, nrow = 1000));\n  df2$female &lt;- rbinom(1000,1,0.5);\n  df2$age &lt;- floor(46*runif(1000) + 20);\n  df2$error &lt;- rnorm(1000,0,0.55);\n  df2$Y0 &lt;- alpha + gamma_f*df2$female + gamma_a*df2$age + df2$error;\n  df2$temp &lt;- runif(1000);\n  df2$treat &lt;- ifelse(df2$temp&gt;=median(df2$temp),1,0);\n  df2$Yobs &lt;- df2$Y0 + tau*df2$treat;\n  lm1 &lt;- lm(Yobs ~ treat, data=df2);\n  lm2 &lt;- lm(Yobs ~ treat + female + age, data=df2);\n  coef &lt;- c(lm1$coefficients[2],lm2$coefficients[2])\n}, simplify = \"array\")\nmat4 &lt;-t(mat3)\ncolnames(mat4)&lt;-c(\"beta1_1000\",\"beta2_1000\")\nsummary(mat4)\n\n   beta1_1000         beta2_1000       \n Min.   :-0.16973   Min.   :-0.114858  \n 1st Qu.:-0.02846   1st Qu.:-0.004257  \n Median : 0.02295   Median : 0.020215  \n Mean   : 0.02234   Mean   : 0.019649  \n 3rd Qu.: 0.07392   3rd Qu.: 0.042947  \n Max.   : 0.26633   Max.   : 0.122476  \n\ndescribe(mat4)\n\n           vars    n mean   sd median trimmed  mad   min  max range  skew\nbeta1_1000    1 1000 0.02 0.08   0.02    0.02 0.08 -0.17 0.27  0.44  0.04\nbeta2_1000    2 1000 0.02 0.04   0.02    0.02 0.03 -0.11 0.12  0.24 -0.02\n           kurtosis se\nbeta1_1000    -0.26  0\nbeta2_1000     0.16  0\n\n\nCalculate power for 1000 obs\n\nse3 &lt;- sqrt((gamma_f^2*(0.5-0.5^2) + gamma_a^2*1/12*(65-20)^2 + 0.55^2)/(1000*(0.5-0.5^2)))\nse4 &lt;- sqrt((0.55^2)/(1000*(0.5-0.5^2)))\nse3\n\n[1] 0.07455803\n\nse4\n\n[1] 0.03478505\n\ndata1$theta3 &lt;- pnorm(qnorm(0.025)-data1$tau0/se3) + 1-pnorm(qnorm(0.975)-data1$tau0/se3)\ndata1$theta4 &lt;- pnorm(qnorm(0.025)-data1$tau0/se4) + 1-pnorm(qnorm(0.975)-data1$tau0/se4)\n\nKernel Density (200 vs 1000)\n\nmat5 &lt;- cbind(mat2,mat4)\nggplot(as.data.frame(mat5), aes(x=beta1)) +\n  geom_density(aes(x=beta1,color=\"Beta1_200\")) +\n  geom_density(aes(x=beta2,color=\"Beta2_200\")) +\n  geom_vline(aes(xintercept=0.02,color=\"tau=0.02\")) +\n  geom_density(aes(x=beta1_1000,color=\"Beta1_1000\")) + \n  geom_density(aes(x=beta2_1000,color=\"Beta2_1000\")) + \n  scale_color_manual(\"\",\n                     breaks=c(\"Beta1_200\",\"Beta2_200\",\"Beta1_1000\",\"Beta2_1000\",\"tau=0.02\"),\n                     values=c(\"Beta1_200\"=\"red\", \"Beta2_200\"=\"blue\",\"tau=0.02\"=\"darkgreen\",\n                              \"Beta1_1000\"=\"orange\",\"Beta2_1000\"=\"purple\")) +\n  xlab(\"beta/tau\") +\n  labs(title=\"kernel density plot\")\n\n\n\n\n\n\n\n\nPlot power curves (200 vs 1000)\n\nggplot(data1, aes(x=tau0,y=theta1)) +\n  geom_line(aes(y=theta1,color=\"theta1_200\")) +\n  geom_line(aes(y=theta2,color=\"theta2_200\")) +\n  geom_line(aes(y=theta3,color=\"theta1_1000\")) +\n  geom_line(aes(y=theta4,color=\"theta2_1000\")) +\n  geom_vline(aes(xintercept=0,color=\"H0\")) +\n  geom_vline(aes(xintercept=0.02,color=\"tau=0.02\")) +\n  geom_hline(aes(yintercept=0.05,color=\"a=0.05\")) +\n  scale_color_manual(\"\",\n                     breaks=c(\"theta1_200\",\"theta2_200\",\"theta1_1000\",\n                              \"theta2_1000\",\"H0\",\"tau=0.02\",\"a=0.05\"),\n                     values=c(\"theta1_200\"=\"red\",\"theta2_200\"=\"blue\",\n                              \"theta1_1000\"=\"orange\",\"theta2_1000\"=\"purple\",\n                              \"H0\"=\"black\",\"tau=0.02\"=\"darkgreen\",\"a=0.05\"=\"darkgrey\")) +\n  xlab(\"beta/tau\") +\n  ylab(\"power\") +\n  labs(title=\"power curves\")"
  },
  {
    "objectID": "seminar-1.html#footnotes",
    "href": "seminar-1.html#footnotes",
    "title": "Seminar 1: Power Calculations and Simulations",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis is not the same as Fisher’s sharp null hypothesis that the unit-level treatment effect is 0 for all units in the data.↩︎\nThose familiar with the Classical Linear Regression Model might find this surprising. Recall, one of the assumption of the CLRM is that the error term is normally distributed. This assumption means that even in a finite sample, the distribution of the estimator (or test statistic) is normally distributed. When the variance of the error term is not known, and must be estimated, the resulting test statistic is T-distributed.↩︎\nIn the limit, we can reduce the probability of a Type I error to 0 by setting the rejection rule to “never reject”. As a result, the probability of a Type II error will be 1 as you will always fail to reject \\(H_0\\) when it is false.↩︎\nYou could use estimates from a previous study.↩︎\nRecall from lectures, the \\(\\hat{\\beta}\\)-OLS estimator for a simple univariate regression model ($Y_i = \\alpha + \\beta W_i + \\varepsilon_i$), is the same as the above \\(\\hat{\\tau}\\) estimator.↩︎\nThis assumption simplifies the simulation by removing any need to consider bias related to heterogeneity.↩︎"
  }
]